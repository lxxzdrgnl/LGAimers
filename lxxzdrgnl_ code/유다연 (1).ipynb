{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\").drop(columns=['ID'])\n",
    "test = pd.read_csv(\"../data/test.csv\").drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('ì„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
    "y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\",\n",
    "    \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\",\n",
    "    \"ì‹œìˆ  ìœ í˜•\",\n",
    "    \"íŠ¹ì • ì‹œìˆ  ìœ í˜•\",\n",
    "    \"ë°°ë€ ìê·¹ ì—¬ë¶€\",\n",
    "    \"ë°°ë€ ìœ ë„ ìœ í˜•\",\n",
    "    \"ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶ˆëª…í™• ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë°°ë€ ì¥ì• \",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ë‚´ë§‰ì¦\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë†ë„\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë©´ì—­í•™ì  ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ìš´ë™ì„±\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì í˜•íƒœ\",\n",
    "    \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
    "    \"ì´ ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"IVF ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"DI ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"ì´ ì„ì‹  íšŸìˆ˜\",\n",
    "    \"IVF ì„ì‹  íšŸìˆ˜\",\n",
    "    \"DI ì„ì‹  íšŸìˆ˜\",\n",
    "    \"ì´ ì¶œì‚° íšŸìˆ˜\",\n",
    "    \"IVF ì¶œì‚° íšŸìˆ˜\",\n",
    "    \"DI ì¶œì‚° íšŸìˆ˜\",\n",
    "    \"ë‚œì ì¶œì²˜\",\n",
    "    \"ì •ì ì¶œì²˜\",\n",
    "    \"ë‚œì ê¸°ì¦ì ë‚˜ì´\",\n",
    "    \"ì •ì ê¸°ì¦ì ë‚˜ì´\",\n",
    "    \"ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ëŒ€ë¦¬ëª¨ ì—¬ë¶€\",\n",
    "    \"PGD ì‹œìˆ  ì—¬ë¶€\",\n",
    "    \"PGS ì‹œìˆ  ì—¬ë¶€\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í…Œê³ ë¦¬í˜• ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜\",\n",
    "    \"ì´ ìƒì„± ë°°ì•„ ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"ì´ì‹ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜\",\n",
    "    \"ì €ì¥ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"í•´ë™ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"í•´ë™ ë‚œì ìˆ˜\",\n",
    "    \"ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜\",\n",
    "    \"ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜\",\n",
    "    \"í˜¼í•©ëœ ë‚œì ìˆ˜\",\n",
    "    \"íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜\",\n",
    "    \"ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜\",\n",
    "    \"ë‚œì ì±„ì·¨ ê²½ê³¼ì¼\",\n",
    "    \"ë‚œì í•´ë™ ê²½ê³¼ì¼\",\n",
    "    \"ë‚œì í˜¼í•© ê²½ê³¼ì¼\",\n",
    "    \"ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼\",\n",
    "    \"ë°°ì•„ í•´ë™ ê²½ê³¼ì¼\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded[numeric_columns] = X_train_encoded[numeric_columns].fillna(0)\n",
    "X_test_encoded[numeric_columns] = X_test_encoded[numeric_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ëœ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "X_train_encoded['ì„ì‹  ì„±ê³µ ì—¬ë¶€'] = y  # íƒ€ê²Ÿ ë³€ìˆ˜ë„ í›ˆë ¨ ë°ì´í„°ì— í¬í•¨ì‹œí‚´\n",
    "X_train_encoded.to_csv('../data/train_encoded.csv', index=False)  # í›ˆë ¨ ë°ì´í„° ì €ì¥\n",
    "\n",
    "X_test_encoded.to_csv('../data/test_encoded.csv', index=False)  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;ExtraTreesClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier(random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_encoded, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = model.predict_proba(X_test_encoded)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì•™ìƒë¸¡ ëª¨ë¸ ì ìš© í›„ ì„±ëŠ¥í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 53102, number of negative: 151978\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 716\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "Confusion Matrix\n",
      "[[35869  2276]\n",
      " [11035  2091]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84     38145\n",
      "           1       0.48      0.16      0.24     13126\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.62      0.55      0.54     51271\n",
      "weighted avg       0.69      0.74      0.69     51271\n",
      "\n",
      "ì˜ˆì¸¡ ê²°ê³¼ê°€ 'baseline_submit.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ íŠ¹ì„± ë¶„ë¦¬\n",
    "y = X_train_encoded['ì„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
    "X = X_train_encoded.drop('ì„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ë‚˜ëˆ„ê¸°\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©)\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# ì•™ìƒë¸” ëª¨ë¸ ì •ì˜ (Voting Classifier)\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('et', et_model),\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "], voting='soft')  # soft voting: ì˜ˆì¸¡ í™•ë¥ ì˜ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred = voting_model.predict(X_val)\n",
    "y_pred_proba = voting_model.predict_proba(X_val)[:, 1]  # ì˜ˆì¸¡ í™•ë¥ \n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ sample_submission íŒŒì¼ì— ì €ì¥\n",
    "pred_proba = voting_model.predict_proba(X_test_encoded)[:, 1]\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ë¡œ ì €ì¥\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ 'baseline_submit.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smote ì ìš© ì½”ë“œ(ì•„ì£¼ ì¡°ê¸ˆ ì˜¤ë¦„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ íŠ¹ì„± ë¶„ë¦¬\n",
    "y = X_train_encoded['ì„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
    "X = X_train_encoded.drop('ì„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë‚˜ëˆ„ê¸° (í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ë‘ )\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# SMOTE ì ìš© (í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì˜¤ë²„ìƒ˜í”Œë§ ì ìš©)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# ë°ì´í„° í¬ê¸° ì¡°ì • (í•„ìš”ì‹œ)\n",
    "target_size = 150000\n",
    "if len(X_train_resampled) > target_size:\n",
    "    X_train_resampled = X_train_resampled[:target_size]\n",
    "    y_train_resampled = y_train_resampled[:target_size]\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ ì •ì˜\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "param_grid_et = {'n_estimators': [100, 200]}\n",
    "grid_et = GridSearchCV(et_model, param_grid_et, cv=3, n_jobs=4, verbose=2)\n",
    "grid_et.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_rf = {'n_estimators': [100, 200]}\n",
    "grid_rf = GridSearchCV(rf_model, param_grid_rf, cv=3, n_jobs=4, verbose=2)\n",
    "grid_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_xgb = {'max_depth': [3, 5]}\n",
    "grid_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=3, n_jobs=4, verbose=2)\n",
    "grid_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_lgb = {'num_leaves': [31, 50]}\n",
    "grid_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=3, n_jobs=4, verbose=2)\n",
    "grid_lgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ë¡œ VotingClassifier ìƒì„±\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('et', grid_et.best_estimator_),\n",
    "    ('rf', grid_rf.best_estimator_),\n",
    "    ('xgb', grid_xgb.best_estimator_),\n",
    "    ('lgb', grid_lgb.best_estimator_)\n",
    "], voting='soft')\n",
    "\n",
    "# ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ\n",
    "voting_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡\n",
    "y_pred = voting_model.predict(X_val)\n",
    "y_pred_proba = voting_model.predict_proba(X_val)[:, 1]  # ì˜ˆì¸¡ í™•ë¥ \n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "pred_proba = voting_model.predict_proba(X_test_encoded)[:, 1]\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "print(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ 'baseline_submit.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ 5:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¬´ì§€ì„± ì´ê³µê²©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ íŠ¹ì„± ë¶„ë¦¬\n",
    "y = X_train_encoded['ì„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
    "X = X_train_encoded.drop('ì„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  (í›ˆë ¨ / ê²€ì¦)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ (ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•´ ì •ê·œí™”)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:29:53,136] A new study created in memory with name: no-name-24a7f11c-60cb-42d2-8d46-926c4a93214d\n",
      "[I 2025-02-18 02:29:56,651] Trial 0 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 169, 'max_depth': 7, 'learning_rate': 0.04025297364106417, 'subsample': 0.7358166445724317, 'colsample_bytree': 0.8102269475077959}. Best is trial 0 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:30:01,916] Trial 1 finished with value: 0.7403015349807883 and parameters: {'n_estimators': 416, 'max_depth': 5, 'learning_rate': 0.22208770832618788, 'subsample': 0.6183057381008377, 'colsample_bytree': 0.5133767447974953}. Best is trial 0 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:30:03,334] Trial 2 finished with value: 0.7450020479413314 and parameters: {'n_estimators': 101, 'max_depth': 4, 'learning_rate': 0.06805186805938937, 'subsample': 0.8343857611064573, 'colsample_bytree': 0.6422163723864018}. Best is trial 0 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:30:11,675] Trial 3 finished with value: 0.7426810477657936 and parameters: {'n_estimators': 382, 'max_depth': 10, 'learning_rate': 0.03727038002477136, 'subsample': 0.8137500609377519, 'colsample_bytree': 0.5716131882450527}. Best is trial 0 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:30:16,992] Trial 4 finished with value: 0.7440268377835424 and parameters: {'n_estimators': 377, 'max_depth': 3, 'learning_rate': 0.011645878738666732, 'subsample': 0.9073171345912994, 'colsample_bytree': 0.8494979304520514}. Best is trial 0 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:30:20,341] Trial 5 finished with value: 0.745743207661251 and parameters: {'n_estimators': 241, 'max_depth': 4, 'learning_rate': 0.051911971361894765, 'subsample': 0.6403870290473936, 'colsample_bytree': 0.92047406198912}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:25,638] Trial 6 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 280, 'max_depth': 7, 'learning_rate': 0.01907403356849584, 'subsample': 0.7310354239521082, 'colsample_bytree': 0.8662117081517948}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:28,688] Trial 7 finished with value: 0.7418033586237834 and parameters: {'n_estimators': 112, 'max_depth': 8, 'learning_rate': 0.01057796319402374, 'subsample': 0.7028756571519222, 'colsample_bytree': 0.5419453541162709}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:32,354] Trial 8 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 241, 'max_depth': 4, 'learning_rate': 0.1508719900889655, 'subsample': 0.6944602338337588, 'colsample_bytree': 0.5783093221301023}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:34,930] Trial 9 finished with value: 0.7419984006553412 and parameters: {'n_estimators': 140, 'max_depth': 8, 'learning_rate': 0.18330909536414017, 'subsample': 0.8041329874105638, 'colsample_bytree': 0.9381447962237659}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:41,883] Trial 10 finished with value: 0.7447094848939947 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.07037556358388808, 'subsample': 0.5121139839665998, 'colsample_bytree': 0.9997439672294715}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:46,135] Trial 11 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 204, 'max_depth': 6, 'learning_rate': 0.03911901384093781, 'subsample': 0.600475093337612, 'colsample_bytree': 0.7525549197625262}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:50,427] Trial 12 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 199, 'max_depth': 7, 'learning_rate': 0.031082851407511543, 'subsample': 0.9909906153059791, 'colsample_bytree': 0.7713448647506738}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:30:57,233] Trial 13 finished with value: 0.7371808624758636 and parameters: {'n_estimators': 309, 'max_depth': 10, 'learning_rate': 0.09850443109608256, 'subsample': 0.627926866027711, 'colsample_bytree': 0.8410837557838045}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:00,159] Trial 14 finished with value: 0.7427590645784167 and parameters: {'n_estimators': 172, 'max_depth': 3, 'learning_rate': 0.01867733469498838, 'subsample': 0.5654458494349022, 'colsample_bytree': 0.6752418994652809}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:04,754] Trial 15 finished with value: 0.7445144428624368 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.10396948415497605, 'subsample': 0.6693584085271644, 'colsample_bytree': 0.9345176245544721}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:11,048] Trial 16 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 313, 'max_depth': 8, 'learning_rate': 0.025870612105091954, 'subsample': 0.7743605161635096, 'colsample_bytree': 0.8040320156730733}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:16,032] Trial 17 finished with value: 0.7433636948762459 and parameters: {'n_estimators': 221, 'max_depth': 9, 'learning_rate': 0.051244506560146015, 'subsample': 0.5049211610851123, 'colsample_bytree': 0.693304569477707}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:19,150] Trial 18 finished with value: 0.745294610988668 and parameters: {'n_estimators': 158, 'max_depth': 5, 'learning_rate': 0.05076555958189294, 'subsample': 0.8680974261021837, 'colsample_bytree': 0.9012058193295129}. Best is trial 5 with value: 0.745743207661251.\n",
      "[I 2025-02-18 02:31:22,728] Trial 19 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.09681040383979617, 'subsample': 0.7481684127880879, 'colsample_bytree': 0.9995661813659329}. Best is trial 19 with value: 0.7457822160675626.\n",
      "[I 2025-02-18 02:31:22,731] A new study created in memory with name: no-name-9dec1127-e34c-405a-90ba-2f6876e71932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ìµœì  XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.09681040383979617, 'subsample': 0.7481684127880879, 'colsample_bytree': 0.9995661813659329}\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:24,426] Trial 0 finished with value: 0.7434807200951805 and parameters: {'n_estimators': 111, 'num_leaves': 85, 'learning_rate': 0.17264050558059515, 'subsample': 0.6452574686688055, 'colsample_bytree': 0.7180151039239253}. Best is trial 0 with value: 0.7434807200951805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:26,799] Trial 1 finished with value: 0.745119073160266 and parameters: {'n_estimators': 161, 'num_leaves': 90, 'learning_rate': 0.02629543092828785, 'subsample': 0.8070970900208738, 'colsample_bytree': 0.8684839212354779}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:29,429] Trial 2 finished with value: 0.7436367537204267 and parameters: {'n_estimators': 441, 'num_leaves': 25, 'learning_rate': 0.14025020572130434, 'subsample': 0.9463959075594561, 'colsample_bytree': 0.851007535645365}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:33,095] Trial 3 finished with value: 0.7403600475902558 and parameters: {'n_estimators': 317, 'num_leaves': 77, 'learning_rate': 0.17562469825527746, 'subsample': 0.8422190927176771, 'colsample_bytree': 0.8982945738690612}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:35,904] Trial 4 finished with value: 0.745119073160266 and parameters: {'n_estimators': 200, 'num_leaves': 60, 'learning_rate': 0.02024788737233216, 'subsample': 0.9170964371080849, 'colsample_bytree': 0.6807222603161285}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:38,801] Trial 5 finished with value: 0.7432466696573111 and parameters: {'n_estimators': 299, 'num_leaves': 41, 'learning_rate': 0.13278207848314738, 'subsample': 0.8407685785677177, 'colsample_bytree': 0.7367177844670243}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:40,868] Trial 6 finished with value: 0.7431491486415323 and parameters: {'n_estimators': 150, 'num_leaves': 31, 'learning_rate': 0.01256623703601442, 'subsample': 0.5608755781534105, 'colsample_bytree': 0.7250244762368446}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:42,714] Trial 7 finished with value: 0.7418813754364065 and parameters: {'n_estimators': 114, 'num_leaves': 44, 'learning_rate': 0.011806045903092265, 'subsample': 0.9114730896902161, 'colsample_bytree': 0.8375538171604662}. Best is trial 1 with value: 0.745119073160266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:45,857] Trial 8 finished with value: 0.7453531235981354 and parameters: {'n_estimators': 411, 'num_leaves': 21, 'learning_rate': 0.02781795865762819, 'subsample': 0.5128407934811927, 'colsample_bytree': 0.8533166730095338}. Best is trial 8 with value: 0.7453531235981354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:48,461] Trial 9 finished with value: 0.7384096272746777 and parameters: {'n_estimators': 215, 'num_leaves': 78, 'learning_rate': 0.27945170316361523, 'subsample': 0.8663858648280014, 'colsample_bytree': 0.7843723084747182}. Best is trial 8 with value: 0.7453531235981354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:54,243] Trial 10 finished with value: 0.7449825437381756 and parameters: {'n_estimators': 498, 'num_leaves': 55, 'learning_rate': 0.0532440884427808, 'subsample': 0.5091019602522958, 'colsample_bytree': 0.5344241375391083}. Best is trial 8 with value: 0.7453531235981354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:31:58,986] Trial 11 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 385, 'num_leaves': 99, 'learning_rate': 0.031169475866953948, 'subsample': 0.7253237702472393, 'colsample_bytree': 0.982537576699871}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:03,737] Trial 12 finished with value: 0.7440658461898539 and parameters: {'n_estimators': 369, 'num_leaves': 99, 'learning_rate': 0.04599100066797926, 'subsample': 0.6960870652731397, 'colsample_bytree': 0.9929391826278574}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:08,187] Trial 13 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 392, 'num_leaves': 71, 'learning_rate': 0.03222134302828503, 'subsample': 0.6217740499773907, 'colsample_bytree': 0.9926800481637529}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:11,437] Trial 14 finished with value: 0.7444364260498137 and parameters: {'n_estimators': 336, 'num_leaves': 69, 'learning_rate': 0.07954357228588457, 'subsample': 0.6306035099563012, 'colsample_bytree': 0.9885470262037059}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:14,905] Trial 15 finished with value: 0.7446704764876831 and parameters: {'n_estimators': 278, 'num_leaves': 96, 'learning_rate': 0.035823678579926396, 'subsample': 0.7481793300747495, 'colsample_bytree': 0.9334929224632396}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:18,787] Trial 16 finished with value: 0.7428565855941955 and parameters: {'n_estimators': 413, 'num_leaves': 69, 'learning_rate': 0.0841770827861587, 'subsample': 0.7083976720436048, 'colsample_bytree': 0.9341598072119791}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:25,444] Trial 17 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 473, 'num_leaves': 85, 'learning_rate': 0.01708435686007513, 'subsample': 0.6037958538911858, 'colsample_bytree': 0.6227309195809221}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:32,018] Trial 18 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 500, 'num_leaves': 87, 'learning_rate': 0.01758608835803874, 'subsample': 0.5800243277198497, 'colsample_bytree': 0.6189078037720122}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:39,710] Trial 19 finished with value: 0.7454116362076028 and parameters: {'n_estimators': 457, 'num_leaves': 100, 'learning_rate': 0.016526779623111283, 'subsample': 0.7507845249313518, 'colsample_bytree': 0.5920163642880996}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:44,937] Trial 20 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 359, 'num_leaves': 91, 'learning_rate': 0.010081999268230073, 'subsample': 0.6822704374141915, 'colsample_bytree': 0.6533174787386191}. Best is trial 11 with value: 0.7456651908486279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:51,241] Trial 21 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 457, 'num_leaves': 84, 'learning_rate': 0.018531255298258952, 'subsample': 0.5829663752037351, 'colsample_bytree': 0.6056277610411357}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:32:58,432] Trial 22 finished with value: 0.7450605605507987 and parameters: {'n_estimators': 449, 'num_leaves': 80, 'learning_rate': 0.02255489386084839, 'subsample': 0.5682097925802554, 'colsample_bytree': 0.5062645843739825}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:05,589] Trial 23 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 468, 'num_leaves': 94, 'learning_rate': 0.014934318830193341, 'subsample': 0.7770774001296609, 'colsample_bytree': 0.5733400047761253}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:11,919] Trial 24 finished with value: 0.7445924596750599 and parameters: {'n_estimators': 416, 'num_leaves': 93, 'learning_rate': 0.03614674672049513, 'subsample': 0.7835333043065082, 'colsample_bytree': 0.5798989228996508}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:17,975] Trial 25 finished with value: 0.745567669832849 and parameters: {'n_estimators': 385, 'num_leaves': 95, 'learning_rate': 0.01303298156331703, 'subsample': 0.737488106380452, 'colsample_bytree': 0.5613033475214517}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:24,235] Trial 26 finished with value: 0.7452360983792007 and parameters: {'n_estimators': 438, 'num_leaves': 100, 'learning_rate': 0.02309008491352425, 'subsample': 0.6661460424748558, 'colsample_bytree': 0.6753448355217936}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:29,137] Trial 27 finished with value: 0.7427200561721051 and parameters: {'n_estimators': 474, 'num_leaves': 81, 'learning_rate': 0.06946017951035205, 'subsample': 0.9911827730580203, 'colsample_bytree': 0.7980982249210271}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:33,605] Trial 28 finished with value: 0.7450800647539545 and parameters: {'n_estimators': 346, 'num_leaves': 60, 'learning_rate': 0.04245535261787588, 'subsample': 0.8019247994836873, 'colsample_bytree': 0.5293768637219152}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:39,344] Trial 29 finished with value: 0.7450995689571103 and parameters: {'n_estimators': 418, 'num_leaves': 85, 'learning_rate': 0.015150316851250427, 'subsample': 0.6542889150621617, 'colsample_bytree': 0.7772077522302876}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:43,483] Trial 30 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 256, 'num_leaves': 91, 'learning_rate': 0.02773046626295189, 'subsample': 0.7222782109002744, 'colsample_bytree': 0.6941842428099978}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:50,043] Trial 31 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 472, 'num_leaves': 85, 'learning_rate': 0.018896296925073615, 'subsample': 0.5988193905735228, 'colsample_bytree': 0.6286924385680644}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:33:59,074] Trial 32 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 473, 'num_leaves': 86, 'learning_rate': 0.015410341100387288, 'subsample': 0.5400349097706215, 'colsample_bytree': 0.6215609289856447}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:06,575] Trial 33 finished with value: 0.7453531235981354 and parameters: {'n_estimators': 441, 'num_leaves': 74, 'learning_rate': 0.02366302073148191, 'subsample': 0.6016241653408787, 'colsample_bytree': 0.5636341603342003}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:15,029] Trial 34 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 479, 'num_leaves': 90, 'learning_rate': 0.010265517153040605, 'subsample': 0.7780239190037929, 'colsample_bytree': 0.646494203828384}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:20,911] Trial 35 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 387, 'num_leaves': 95, 'learning_rate': 0.01089964747772424, 'subsample': 0.7759151158879951, 'colsample_bytree': 0.6552701601650205}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:28,329] Trial 36 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 430, 'num_leaves': 89, 'learning_rate': 0.01350316250006279, 'subsample': 0.7893274943565206, 'colsample_bytree': 0.5957757139571845}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:35,167] Trial 37 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 498, 'num_leaves': 65, 'learning_rate': 0.013506771523161218, 'subsample': 0.8589218696821805, 'colsample_bytree': 0.595197982789584}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:40,945] Trial 38 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 490, 'num_leaves': 56, 'learning_rate': 0.01056652506823797, 'subsample': 0.8362944904639444, 'colsample_bytree': 0.7043308708934344}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:47,583] Trial 39 finished with value: 0.7454116362076028 and parameters: {'n_estimators': 458, 'num_leaves': 44, 'learning_rate': 0.013386703556529371, 'subsample': 0.8753210722451314, 'colsample_bytree': 0.5438846055306473}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:34:54,582] Trial 40 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 485, 'num_leaves': 64, 'learning_rate': 0.020015275916187198, 'subsample': 0.8323477687398254, 'colsample_bytree': 0.6581895554573021}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:01,270] Trial 41 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 428, 'num_leaves': 82, 'learning_rate': 0.0137994895732985, 'subsample': 0.8041993227406403, 'colsample_bytree': 0.6000592675224803}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:08,223] Trial 42 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 457, 'num_leaves': 75, 'learning_rate': 0.011890734501141599, 'subsample': 0.8834678832053767, 'colsample_bytree': 0.5061256863554979}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:14,718] Trial 43 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 433, 'num_leaves': 83, 'learning_rate': 0.01445863000620752, 'subsample': 0.8189524830785526, 'colsample_bytree': 0.599896861596631}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:21,712] Trial 44 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 500, 'num_leaves': 65, 'learning_rate': 0.0202192270897744, 'subsample': 0.760193244223844, 'colsample_bytree': 0.567175141947587}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:26,663] Trial 45 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 466, 'num_leaves': 38, 'learning_rate': 0.012018938426777249, 'subsample': 0.8585966397149856, 'colsample_bytree': 0.6393688298235982}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:33,215] Trial 46 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 403, 'num_leaves': 80, 'learning_rate': 0.010392362907073434, 'subsample': 0.9239535047593642, 'colsample_bytree': 0.7370766806634237}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:40,107] Trial 47 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 484, 'num_leaves': 76, 'learning_rate': 0.014525480201282628, 'subsample': 0.8981212990885293, 'colsample_bytree': 0.5436486531321839}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:46,528] Trial 48 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 427, 'num_leaves': 76, 'learning_rate': 0.015613353790826809, 'subsample': 0.8973328629490239, 'colsample_bytree': 0.5405809880761042}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:49,152] Trial 49 finished with value: 0.744416921846658 and parameters: {'n_estimators': 174, 'num_leaves': 53, 'learning_rate': 0.12265008776216911, 'subsample': 0.9592067271844917, 'colsample_bytree': 0.5259903870931695}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:35:54,673] Trial 50 finished with value: 0.7450215521444872 and parameters: {'n_estimators': 444, 'num_leaves': 71, 'learning_rate': 0.02444621894220222, 'subsample': 0.9370460876486955, 'colsample_bytree': 0.606247980475566}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:02,433] Trial 51 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 481, 'num_leaves': 89, 'learning_rate': 0.012608607880147001, 'subsample': 0.8548694219742908, 'colsample_bytree': 0.5801287097899853}. Best is trial 21 with value: 0.7459382496928088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:10,244] Trial 52 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 487, 'num_leaves': 78, 'learning_rate': 0.014415713455676566, 'subsample': 0.8272568552723565, 'colsample_bytree': 0.5605146997460406}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:17,069] Trial 53 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 490, 'num_leaves': 73, 'learning_rate': 0.017392923192912888, 'subsample': 0.8156014939590397, 'colsample_bytree': 0.5582033898910362}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:24,221] Trial 54 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 458, 'num_leaves': 78, 'learning_rate': 0.018534889209986197, 'subsample': 0.898179865842182, 'colsample_bytree': 0.5781225297895135}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:29,110] Trial 55 finished with value: 0.7347233328782352 and parameters: {'n_estimators': 401, 'num_leaves': 82, 'learning_rate': 0.28513554199378094, 'subsample': 0.9095311520003111, 'colsample_bytree': 0.5025953938621127}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:33,865] Trial 56 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 309, 'num_leaves': 78, 'learning_rate': 0.018683633926708083, 'subsample': 0.886952710200873, 'colsample_bytree': 0.5224122849565924}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:36,075] Trial 57 finished with value: 0.743968325174075 and parameters: {'n_estimators': 101, 'num_leaves': 78, 'learning_rate': 0.021483387478920465, 'subsample': 0.7998304095642479, 'colsample_bytree': 0.5455555830928962}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:42,437] Trial 58 finished with value: 0.7450215521444872 and parameters: {'n_estimators': 455, 'num_leaves': 69, 'learning_rate': 0.03144598890171284, 'subsample': 0.766010045510956, 'colsample_bytree': 0.5791849418856495}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:48,644] Trial 59 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 464, 'num_leaves': 88, 'learning_rate': 0.02553266101554474, 'subsample': 0.9680741168629433, 'colsample_bytree': 0.6116823017677084}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:36:54,932] Trial 60 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 366, 'num_leaves': 83, 'learning_rate': 0.0174332547715127, 'subsample': 0.8231606699252006, 'colsample_bytree': 0.5805139570445346}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:00,647] Trial 61 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 368, 'num_leaves': 82, 'learning_rate': 0.01680263710910808, 'subsample': 0.8303282622885464, 'colsample_bytree': 0.5513623958214144}. Best is trial 52 with value: 0.7459772580991204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:07,413] Trial 62 finished with value: 0.7461332917243666 and parameters: {'n_estimators': 424, 'num_leaves': 78, 'learning_rate': 0.014380233261059234, 'subsample': 0.8473493944740542, 'colsample_bytree': 0.5737858499556483}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:12,798] Trial 63 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 337, 'num_leaves': 72, 'learning_rate': 0.020674014811585716, 'subsample': 0.8469827114022132, 'colsample_bytree': 0.5854134026115326}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:17,694] Trial 64 finished with value: 0.737668467554758 and parameters: {'n_estimators': 420, 'num_leaves': 78, 'learning_rate': 0.2214350731311081, 'subsample': 0.9028386708912383, 'colsample_bytree': 0.678065956526868}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:25,595] Trial 65 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 403, 'num_leaves': 83, 'learning_rate': 0.018146149076697066, 'subsample': 0.870317820107285, 'colsample_bytree': 0.5198113039565079}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:30,216] Trial 66 finished with value: 0.7448070059097736 and parameters: {'n_estimators': 282, 'num_leaves': 77, 'learning_rate': 0.01443200228750019, 'subsample': 0.9286645421593944, 'colsample_bytree': 0.6138625335628417}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:37,059] Trial 67 finished with value: 0.745392132004447 and parameters: {'n_estimators': 444, 'num_leaves': 80, 'learning_rate': 0.027965014754434844, 'subsample': 0.8152728399586227, 'colsample_bytree': 0.6290063616196598}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:44,653] Trial 68 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 371, 'num_leaves': 75, 'learning_rate': 0.011386680981298906, 'subsample': 0.5295862731654885, 'colsample_bytree': 0.5561301251457277}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:37:51,972] Trial 69 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 429, 'num_leaves': 67, 'learning_rate': 0.01639132189135817, 'subsample': 0.8929404940978136, 'colsample_bytree': 0.5323003501840792}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:00,020] Trial 70 finished with value: 0.7439878293772308 and parameters: {'n_estimators': 451, 'num_leaves': 92, 'learning_rate': 0.054882085806928535, 'subsample': 0.9497339610392852, 'colsample_bytree': 0.5672385118280062}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:07,648] Trial 71 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 468, 'num_leaves': 86, 'learning_rate': 0.014790171724037177, 'subsample': 0.7938336095286366, 'colsample_bytree': 0.5722996345260264}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:16,973] Trial 72 finished with value: 0.7448655185192409 and parameters: {'n_estimators': 486, 'num_leaves': 94, 'learning_rate': 0.01856975701619275, 'subsample': 0.744216981704032, 'colsample_bytree': 0.5887747755335166}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:23,912] Trial 73 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 443, 'num_leaves': 97, 'learning_rate': 0.014105851315917104, 'subsample': 0.8385955169581973, 'colsample_bytree': 0.6027100187192477}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:27,729] Trial 74 finished with value: 0.7450800647539545 and parameters: {'n_estimators': 226, 'num_leaves': 80, 'learning_rate': 0.012262044480008491, 'subsample': 0.8225246686934493, 'colsample_bytree': 0.8925130680303812}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:35,176] Trial 75 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 413, 'num_leaves': 84, 'learning_rate': 0.022435945174082098, 'subsample': 0.7150245316209302, 'colsample_bytree': 0.6378962829383585}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:45,128] Trial 76 finished with value: 0.7460552749117435 and parameters: {'n_estimators': 476, 'num_leaves': 87, 'learning_rate': 0.01645594435166776, 'subsample': 0.849458000955458, 'colsample_bytree': 0.5172487736250921}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:38:53,256] Trial 77 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 476, 'num_leaves': 87, 'learning_rate': 0.016327015954642428, 'subsample': 0.8758831342877491, 'colsample_bytree': 0.8313373144417937}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:03,095] Trial 78 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 462, 'num_leaves': 84, 'learning_rate': 0.012801023705363405, 'subsample': 0.8471533094605076, 'colsample_bytree': 0.5176861250428932}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:07,892] Trial 79 finished with value: 0.7450020479413314 and parameters: {'n_estimators': 325, 'num_leaves': 27, 'learning_rate': 0.013117556883402182, 'subsample': 0.8469925233494807, 'colsample_bytree': 0.516281229548324}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:15,564] Trial 80 finished with value: 0.745392132004447 and parameters: {'n_estimators': 394, 'num_leaves': 84, 'learning_rate': 0.010906288211082278, 'subsample': 0.8058758555955949, 'colsample_bytree': 0.5376543258717089}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:24,333] Trial 81 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 459, 'num_leaves': 80, 'learning_rate': 0.015715186747867108, 'subsample': 0.8658404362629715, 'colsample_bytree': 0.5001670419897328}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:31,445] Trial 82 finished with value: 0.745743207661251 and parameters: {'n_estimators': 488, 'num_leaves': 76, 'learning_rate': 0.01811206218503495, 'subsample': 0.8477913381450937, 'colsample_bytree': 0.5499166202183012}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:39,468] Trial 83 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 436, 'num_leaves': 87, 'learning_rate': 0.011582335131728594, 'subsample': 0.8311371675507085, 'colsample_bytree': 0.5160413377766249}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:46,620] Trial 84 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 451, 'num_leaves': 82, 'learning_rate': 0.019671373326708787, 'subsample': 0.9123323837706071, 'colsample_bytree': 0.5365861824678619}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:39:55,167] Trial 85 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 465, 'num_leaves': 92, 'learning_rate': 0.012927681127540032, 'subsample': 0.8835265065332637, 'colsample_bytree': 0.5636513451352758}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:04,023] Trial 86 finished with value: 0.745918745489653 and parameters: {'n_estimators': 495, 'num_leaves': 89, 'learning_rate': 0.014417674015908965, 'subsample': 0.8619511689340958, 'colsample_bytree': 0.5892241216683571}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:11,363] Trial 87 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 424, 'num_leaves': 73, 'learning_rate': 0.016916809491784205, 'subsample': 0.8161887350633984, 'colsample_bytree': 0.5754766241891321}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:19,045] Trial 88 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 475, 'num_leaves': 71, 'learning_rate': 0.015398582455722145, 'subsample': 0.7680127430699774, 'colsample_bytree': 0.5137518291588856}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:28,762] Trial 89 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 439, 'num_leaves': 79, 'learning_rate': 0.02149698152341747, 'subsample': 0.7869675800994534, 'colsample_bytree': 0.5515076890971781}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:38,768] Trial 90 finished with value: 0.7445924596750599 and parameters: {'n_estimators': 483, 'num_leaves': 85, 'learning_rate': 0.03496583521523169, 'subsample': 0.8020842921797569, 'colsample_bytree': 0.5302570581375676}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:49,660] Trial 91 finished with value: 0.7454116362076028 and parameters: {'n_estimators': 499, 'num_leaves': 90, 'learning_rate': 0.013719034270339298, 'subsample': 0.8597294551799955, 'colsample_bytree': 0.5918967305277238}. Best is trial 62 with value: 0.7461332917243666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:40:58,987] Trial 92 finished with value: 0.746367342162236 and parameters: {'n_estimators': 492, 'num_leaves': 88, 'learning_rate': 0.014666949438705097, 'subsample': 0.8766412273868395, 'colsample_bytree': 0.5848756135604947}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:06,951] Trial 93 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 465, 'num_leaves': 83, 'learning_rate': 0.012748160261828073, 'subsample': 0.8774242452240572, 'colsample_bytree': 0.6171246447609774}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:14,483] Trial 94 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 451, 'num_leaves': 97, 'learning_rate': 0.011809143060682849, 'subsample': 0.8970786537599598, 'colsample_bytree': 0.6027591426890371}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:21,596] Trial 95 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 479, 'num_leaves': 86, 'learning_rate': 0.01754556745114778, 'subsample': 0.9212327073499691, 'colsample_bytree': 0.5605338343418195}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:29,554] Trial 96 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 489, 'num_leaves': 81, 'learning_rate': 0.019908183839009514, 'subsample': 0.8267741426287838, 'colsample_bytree': 0.5724980426603176}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:37,019] Trial 97 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 460, 'num_leaves': 75, 'learning_rate': 0.015809911739626793, 'subsample': 0.8413373685056583, 'colsample_bytree': 0.5442811974327547}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:43,984] Trial 98 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 378, 'num_leaves': 77, 'learning_rate': 0.014263092670951508, 'subsample': 0.6256786792233217, 'colsample_bytree': 0.5808474473706998}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:51,599] Trial 99 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 471, 'num_leaves': 69, 'learning_rate': 0.011122587353409935, 'subsample': 0.9330746857534431, 'colsample_bytree': 0.5301557305353315}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:41:56,471] Trial 100 finished with value: 0.7428565855941955 and parameters: {'n_estimators': 411, 'num_leaves': 62, 'learning_rate': 0.09894318757732243, 'subsample': 0.695107495037085, 'colsample_bytree': 0.6312580183194433}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:04,196] Trial 101 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 497, 'num_leaves': 89, 'learning_rate': 0.014211204293082013, 'subsample': 0.8631054460651312, 'colsample_bytree': 0.5887678095148293}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:11,501] Trial 102 finished with value: 0.7460357707085877 and parameters: {'n_estimators': 494, 'num_leaves': 91, 'learning_rate': 0.014702556359641293, 'subsample': 0.8511867614526036, 'colsample_bytree': 0.9647114196413047}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:18,478] Trial 103 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 480, 'num_leaves': 91, 'learning_rate': 0.01910445070142195, 'subsample': 0.8530571478162327, 'colsample_bytree': 0.7439472528160949}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:24,792] Trial 104 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 448, 'num_leaves': 87, 'learning_rate': 0.016807147297255864, 'subsample': 0.9030450169487793, 'colsample_bytree': 0.9287987493009195}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:33,000] Trial 105 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 493, 'num_leaves': 84, 'learning_rate': 0.012270972010756883, 'subsample': 0.8702182623066007, 'colsample_bytree': 0.9726534573449754}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:38,779] Trial 106 finished with value: 0.7451775857697334 and parameters: {'n_estimators': 433, 'num_leaves': 82, 'learning_rate': 0.015150251019472956, 'subsample': 0.8859190326409833, 'colsample_bytree': 0.7968140142937581}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:45,381] Trial 107 finished with value: 0.745567669832849 and parameters: {'n_estimators': 356, 'num_leaves': 79, 'learning_rate': 0.010276375724128891, 'subsample': 0.8387806929198344, 'colsample_bytree': 0.554588887292969}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:42:53,910] Trial 108 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 472, 'num_leaves': 93, 'learning_rate': 0.024033884213654558, 'subsample': 0.8135833621979935, 'colsample_bytree': 0.5104559811719772}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:00,814] Trial 109 finished with value: 0.745041056347643 and parameters: {'n_estimators': 459, 'num_leaves': 85, 'learning_rate': 0.013532654407675598, 'subsample': 0.8524474482940326, 'colsample_bytree': 0.8316629019024506}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:03,284] Trial 110 finished with value: 0.7443194008308791 and parameters: {'n_estimators': 135, 'num_leaves': 74, 'learning_rate': 0.022094825427290677, 'subsample': 0.6375839555372372, 'colsample_bytree': 0.6947555125978297}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:10,740] Trial 111 finished with value: 0.745918745489653 and parameters: {'n_estimators': 492, 'num_leaves': 89, 'learning_rate': 0.014841647568803885, 'subsample': 0.8748215055736284, 'colsample_bytree': 0.6079350263348056}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:19,326] Trial 112 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 479, 'num_leaves': 88, 'learning_rate': 0.01774731822825481, 'subsample': 0.8332937263380136, 'colsample_bytree': 0.6631248276367105}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:26,599] Trial 113 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 486, 'num_leaves': 91, 'learning_rate': 0.016060932963274537, 'subsample': 0.8904470003741812, 'colsample_bytree': 0.5958828555920749}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:34,552] Trial 114 finished with value: 0.7448460143160851 and parameters: {'n_estimators': 470, 'num_leaves': 83, 'learning_rate': 0.013690015463444246, 'subsample': 0.8638754433100009, 'colsample_bytree': 0.7667788032761713}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:41,891] Trial 115 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 499, 'num_leaves': 81, 'learning_rate': 0.012596229637498492, 'subsample': 0.9075421680709639, 'colsample_bytree': 0.5706632179575564}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:47,842] Trial 116 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 454, 'num_leaves': 51, 'learning_rate': 0.014668055755041601, 'subsample': 0.8231537070178297, 'colsample_bytree': 0.6209288100825792}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:43:55,553] Trial 117 finished with value: 0.7454116362076028 and parameters: {'n_estimators': 483, 'num_leaves': 96, 'learning_rate': 0.01107291351904061, 'subsample': 0.7937292062592253, 'colsample_bytree': 0.7206620395875765}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:02,690] Trial 118 finished with value: 0.745392132004447 and parameters: {'n_estimators': 462, 'num_leaves': 87, 'learning_rate': 0.01663582557011276, 'subsample': 0.8098415085820581, 'colsample_bytree': 0.5822985602402595}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:10,596] Trial 119 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 491, 'num_leaves': 78, 'learning_rate': 0.018772243293690788, 'subsample': 0.8555924206099188, 'colsample_bytree': 0.5251086061730827}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:16,487] Trial 120 finished with value: 0.7446509722845274 and parameters: {'n_estimators': 443, 'num_leaves': 85, 'learning_rate': 0.04744171280275072, 'subsample': 0.8451678685470172, 'colsample_bytree': 0.540982118093968}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:23,551] Trial 121 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 475, 'num_leaves': 89, 'learning_rate': 0.015085183493519796, 'subsample': 0.8766991592977292, 'colsample_bytree': 0.8638793532815919}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:31,906] Trial 122 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 494, 'num_leaves': 93, 'learning_rate': 0.013209441811123775, 'subsample': 0.8700957140648676, 'colsample_bytree': 0.5983107045823872}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:41,349] Trial 123 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 492, 'num_leaves': 93, 'learning_rate': 0.013348484989929528, 'subsample': 0.8650461720076488, 'colsample_bytree': 0.5996637310575734}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:49,680] Trial 124 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 484, 'num_leaves': 94, 'learning_rate': 0.011927929303763064, 'subsample': 0.8844501352328252, 'colsample_bytree': 0.5654716098184077}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:44:57,744] Trial 125 finished with value: 0.7451385773634218 and parameters: {'n_estimators': 473, 'num_leaves': 98, 'learning_rate': 0.020775218032430487, 'subsample': 0.8972516130000239, 'colsample_bytree': 0.5878884279410423}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:05,194] Trial 126 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 499, 'num_leaves': 90, 'learning_rate': 0.012856122547689437, 'subsample': 0.5832307705026212, 'colsample_bytree': 0.612637783310202}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:12,714] Trial 127 finished with value: 0.7453726278012912 and parameters: {'n_estimators': 423, 'num_leaves': 76, 'learning_rate': 0.01584276618238558, 'subsample': 0.914816098657402, 'colsample_bytree': 0.5577649477337563}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:19,532] Trial 128 finished with value: 0.745119073160266 and parameters: {'n_estimators': 466, 'num_leaves': 80, 'learning_rate': 0.017734850975783395, 'subsample': 0.8276502629551009, 'colsample_bytree': 0.579331699430856}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:25,701] Trial 129 finished with value: 0.7429346024068186 and parameters: {'n_estimators': 485, 'num_leaves': 86, 'learning_rate': 0.06537158552101417, 'subsample': 0.8409389374160972, 'colsample_bytree': 0.6357608180832734}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:31,210] Trial 130 finished with value: 0.7449630395350197 and parameters: {'n_estimators': 453, 'num_leaves': 82, 'learning_rate': 0.026773760204344434, 'subsample': 0.8461921796775231, 'colsample_bytree': 0.6504651335497983}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:38,468] Trial 131 finished with value: 0.745567669832849 and parameters: {'n_estimators': 488, 'num_leaves': 88, 'learning_rate': 0.014537419513847702, 'subsample': 0.8744272187532228, 'colsample_bytree': 0.6004757974982309}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:46,226] Trial 132 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 492, 'num_leaves': 89, 'learning_rate': 0.014243041831423442, 'subsample': 0.8557587358115082, 'colsample_bytree': 0.6044045331491963}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:51,673] Trial 133 finished with value: 0.7370248288506173 and parameters: {'n_estimators': 500, 'num_leaves': 92, 'learning_rate': 0.16342429563491273, 'subsample': 0.8813756931744412, 'colsample_bytree': 0.6104807280918711}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:45:59,193] Trial 134 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 477, 'num_leaves': 95, 'learning_rate': 0.01561279655072593, 'subsample': 0.8679296664743094, 'colsample_bytree': 0.6224092581192697}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:06,121] Trial 135 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 466, 'num_leaves': 83, 'learning_rate': 0.011901229107125051, 'subsample': 0.941883518333011, 'colsample_bytree': 0.5499148736963416}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:13,545] Trial 136 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 478, 'num_leaves': 84, 'learning_rate': 0.016695636229571872, 'subsample': 0.8908881548292299, 'colsample_bytree': 0.5721953646260489}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:20,115] Trial 137 finished with value: 0.7461137875212108 and parameters: {'n_estimators': 459, 'num_leaves': 86, 'learning_rate': 0.013368476615220266, 'subsample': 0.6698000559355917, 'colsample_bytree': 0.5884094107869975}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:26,446] Trial 138 finished with value: 0.7461527959275224 and parameters: {'n_estimators': 448, 'num_leaves': 79, 'learning_rate': 0.013338372505131629, 'subsample': 0.5450082228551895, 'colsample_bytree': 0.587459451964639}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:33,429] Trial 139 finished with value: 0.7453726278012912 and parameters: {'n_estimators': 437, 'num_leaves': 79, 'learning_rate': 0.011280347415360223, 'subsample': 0.5300325390892355, 'colsample_bytree': 0.5614380283617243}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:40,162] Trial 140 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 446, 'num_leaves': 76, 'learning_rate': 0.013043049826924058, 'subsample': 0.67164807530961, 'colsample_bytree': 0.5439426383837479}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:47,823] Trial 141 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 459, 'num_leaves': 86, 'learning_rate': 0.013841160616137715, 'subsample': 0.561961428309677, 'colsample_bytree': 0.5891731254107438}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:46:53,851] Trial 142 finished with value: 0.745743207661251 and parameters: {'n_estimators': 431, 'num_leaves': 81, 'learning_rate': 0.01252122547945218, 'subsample': 0.5532384513052637, 'colsample_bytree': 0.5800609372129485}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:00,511] Trial 143 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 468, 'num_leaves': 78, 'learning_rate': 0.015120619223926816, 'subsample': 0.5856631763432218, 'colsample_bytree': 0.5901043758360246}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:05,460] Trial 144 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 290, 'num_leaves': 91, 'learning_rate': 0.013469039370143945, 'subsample': 0.5187776063714127, 'colsample_bytree': 0.5683230870325137}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:11,897] Trial 145 finished with value: 0.7460357707085877 and parameters: {'n_estimators': 449, 'num_leaves': 84, 'learning_rate': 0.01712852663453045, 'subsample': 0.7334846860210658, 'colsample_bytree': 0.597906168490943}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:16,101] Trial 146 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 248, 'num_leaves': 83, 'learning_rate': 0.01817644669014771, 'subsample': 0.615428427581654, 'colsample_bytree': 0.5956126387015558}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:22,957] Trial 147 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 402, 'num_leaves': 81, 'learning_rate': 0.019490180978937406, 'subsample': 0.7354579178717193, 'colsample_bytree': 0.5343216157825564}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:29,258] Trial 148 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 420, 'num_leaves': 85, 'learning_rate': 0.017287484375498735, 'subsample': 0.5476441447450991, 'colsample_bytree': 0.5199852123683923}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:36,356] Trial 149 finished with value: 0.7452165941760449 and parameters: {'n_estimators': 438, 'num_leaves': 73, 'learning_rate': 0.010023818287225176, 'subsample': 0.6902518444925976, 'colsample_bytree': 0.5764398452912678}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:42,938] Trial 150 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 459, 'num_leaves': 79, 'learning_rate': 0.016250214117174464, 'subsample': 0.6513208756105893, 'colsample_bytree': 0.5568470826673976}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:49,246] Trial 151 finished with value: 0.7459577538959645 and parameters: {'n_estimators': 453, 'num_leaves': 85, 'learning_rate': 0.014004116511555071, 'subsample': 0.8336828952541907, 'colsample_bytree': 0.9727942141107886}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:47:55,553] Trial 152 finished with value: 0.7450605605507987 and parameters: {'n_estimators': 451, 'num_leaves': 87, 'learning_rate': 0.01232323369906881, 'subsample': 0.5711135319329257, 'colsample_bytree': 0.9353892696007394}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:01,561] Trial 153 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 445, 'num_leaves': 83, 'learning_rate': 0.015461070209382725, 'subsample': 0.5062680664111028, 'colsample_bytree': 0.6238557415054676}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:08,476] Trial 154 finished with value: 0.7452165941760449 and parameters: {'n_estimators': 453, 'num_leaves': 85, 'learning_rate': 0.01425386248019885, 'subsample': 0.8255431177096915, 'colsample_bytree': 0.9996956682034519}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:13,950] Trial 155 finished with value: 0.7450215521444872 and parameters: {'n_estimators': 412, 'num_leaves': 77, 'learning_rate': 0.01324454220732538, 'subsample': 0.8366452075924895, 'colsample_bytree': 0.9514863730549286}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:19,748] Trial 156 finished with value: 0.7449240311287082 and parameters: {'n_estimators': 427, 'num_leaves': 81, 'learning_rate': 0.011799510946947946, 'subsample': 0.8020401810416509, 'colsample_bytree': 0.9721438890513243}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:26,040] Trial 157 finished with value: 0.7448460143160851 and parameters: {'n_estimators': 467, 'num_leaves': 88, 'learning_rate': 0.016548630189712286, 'subsample': 0.7191279297370334, 'colsample_bytree': 0.8148048901600776}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:31,669] Trial 158 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 441, 'num_leaves': 84, 'learning_rate': 0.013865182676761604, 'subsample': 0.6639920149060133, 'colsample_bytree': 0.9616623284509664}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:39,764] Trial 159 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 458, 'num_leaves': 86, 'learning_rate': 0.010866088471324475, 'subsample': 0.773485254154063, 'colsample_bytree': 0.5036766589641497}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:45,453] Trial 160 finished with value: 0.7451970899728891 and parameters: {'n_estimators': 472, 'num_leaves': 79, 'learning_rate': 0.01900712332982742, 'subsample': 0.8178595367270791, 'colsample_bytree': 0.8979582996289028}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:53,013] Trial 161 finished with value: 0.7461332917243666 and parameters: {'n_estimators': 480, 'num_leaves': 90, 'learning_rate': 0.014980485997596257, 'subsample': 0.847611389445964, 'colsample_bytree': 0.5834186538487102}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:48:59,596] Trial 162 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 482, 'num_leaves': 92, 'learning_rate': 0.015333841024508415, 'subsample': 0.8333488395497246, 'colsample_bytree': 0.9865329848066633}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:06,799] Trial 163 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 462, 'num_leaves': 90, 'learning_rate': 0.012812905450347552, 'subsample': 0.8483017849519464, 'colsample_bytree': 0.5847353582815467}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:12,361] Trial 164 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 320, 'num_leaves': 87, 'learning_rate': 0.014628583128922908, 'subsample': 0.8549850697741642, 'colsample_bytree': 0.6033448674216665}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:19,101] Trial 165 finished with value: 0.745743207661251 and parameters: {'n_estimators': 478, 'num_leaves': 82, 'learning_rate': 0.016771983597890037, 'subsample': 0.8401463079192882, 'colsample_bytree': 0.5658812394995802}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:25,908] Trial 166 finished with value: 0.7460747791148993 and parameters: {'n_estimators': 448, 'num_leaves': 80, 'learning_rate': 0.017735696142805155, 'subsample': 0.8095483837170487, 'colsample_bytree': 0.6144382904599592}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:32,231] Trial 167 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 449, 'num_leaves': 85, 'learning_rate': 0.01763472286210721, 'subsample': 0.793396500391563, 'colsample_bytree': 0.5932393874124581}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:38,884] Trial 168 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 432, 'num_leaves': 88, 'learning_rate': 0.01567835038891547, 'subsample': 0.8115967253912854, 'colsample_bytree': 0.6163209407446683}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:44,800] Trial 169 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 392, 'num_leaves': 80, 'learning_rate': 0.014195389475710604, 'subsample': 0.7840876353804225, 'colsample_bytree': 0.8848615677423793}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:51,344] Trial 170 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 471, 'num_leaves': 84, 'learning_rate': 0.013477614898900436, 'subsample': 0.7577576554424311, 'colsample_bytree': 0.6126884788355318}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:49:58,221] Trial 171 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 456, 'num_leaves': 77, 'learning_rate': 0.020985617649061573, 'subsample': 0.8246972833163638, 'colsample_bytree': 0.5754162883610744}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:04,296] Trial 172 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 445, 'num_leaves': 75, 'learning_rate': 0.0160110808049912, 'subsample': 0.848206609399142, 'colsample_bytree': 0.5995325825295597}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:10,484] Trial 173 finished with value: 0.745918745489653 and parameters: {'n_estimators': 443, 'num_leaves': 75, 'learning_rate': 0.015021844363593495, 'subsample': 0.8496593319057614, 'colsample_bytree': 0.5967778039661067}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:17,632] Trial 174 finished with value: 0.7453531235981354 and parameters: {'n_estimators': 486, 'num_leaves': 82, 'learning_rate': 0.015863971485268066, 'subsample': 0.608652484376635, 'colsample_bytree': 0.6420326833573287}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:23,739] Trial 175 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 433, 'num_leaves': 80, 'learning_rate': 0.012274200542055014, 'subsample': 0.8066132317345673, 'colsample_bytree': 0.6045686544680954}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:26,563] Trial 176 finished with value: 0.7448070059097736 and parameters: {'n_estimators': 183, 'num_leaves': 57, 'learning_rate': 0.017214942218531343, 'subsample': 0.8333379585332824, 'colsample_bytree': 0.6291021468172814}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:33,809] Trial 177 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 449, 'num_leaves': 90, 'learning_rate': 0.013545088519875858, 'subsample': 0.7037865185864208, 'colsample_bytree': 0.5832505264861823}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:40,102] Trial 178 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 463, 'num_leaves': 77, 'learning_rate': 0.014748089916719834, 'subsample': 0.5950439872444974, 'colsample_bytree': 0.609631897831553}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:48,772] Trial 179 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 480, 'num_leaves': 94, 'learning_rate': 0.012672081128140809, 'subsample': 0.8627830373575127, 'colsample_bytree': 0.5475739567324723}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:50:55,499] Trial 180 finished with value: 0.7455286614265374 and parameters: {'n_estimators': 491, 'num_leaves': 84, 'learning_rate': 0.018029943106764017, 'subsample': 0.8199904003116084, 'colsample_bytree': 0.5976152788613148}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:02,633] Trial 181 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 439, 'num_leaves': 78, 'learning_rate': 0.019607771480863588, 'subsample': 0.8459106835575454, 'colsample_bytree': 0.5725435675385905}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:08,644] Trial 182 finished with value: 0.745918745489653 and parameters: {'n_estimators': 347, 'num_leaves': 71, 'learning_rate': 0.01615358402246598, 'subsample': 0.8677096877251764, 'colsample_bytree': 0.5845131938214777}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:14,506] Trial 183 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 472, 'num_leaves': 37, 'learning_rate': 0.018161128757552908, 'subsample': 0.8591376596427481, 'colsample_bytree': 0.5642652224586608}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:21,849] Trial 184 finished with value: 0.746094283318055 and parameters: {'n_estimators': 457, 'num_leaves': 74, 'learning_rate': 0.014015052018077678, 'subsample': 0.8404162268496809, 'colsample_bytree': 0.5920980181239401}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:28,301] Trial 185 finished with value: 0.7460357707085877 and parameters: {'n_estimators': 454, 'num_leaves': 73, 'learning_rate': 0.014002716490237795, 'subsample': 0.8376708956102936, 'colsample_bytree': 0.5902181554536556}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:36,063] Trial 186 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 452, 'num_leaves': 86, 'learning_rate': 0.011377155482984479, 'subsample': 0.8229803605187889, 'colsample_bytree': 0.5985453482824271}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:41,964] Trial 187 finished with value: 0.7455286614265374 and parameters: {'n_estimators': 423, 'num_leaves': 74, 'learning_rate': 0.013876290874452758, 'subsample': 0.8352135412325192, 'colsample_bytree': 0.6175666139616933}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:48,863] Trial 188 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 444, 'num_leaves': 73, 'learning_rate': 0.012832547787412472, 'subsample': 0.842324557147167, 'colsample_bytree': 0.5850721563791362}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:51:55,335] Trial 189 finished with value: 0.7451385773634218 and parameters: {'n_estimators': 464, 'num_leaves': 68, 'learning_rate': 0.01471090908263428, 'subsample': 0.8530323932934242, 'colsample_bytree': 0.608705660411101}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:02,059] Trial 190 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 456, 'num_leaves': 75, 'learning_rate': 0.01601702219778048, 'subsample': 0.8308209758188247, 'colsample_bytree': 0.5889831620592701}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:09,823] Trial 191 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 467, 'num_leaves': 70, 'learning_rate': 0.013922408227012092, 'subsample': 0.7303917746709495, 'colsample_bytree': 0.5762450974274733}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:16,047] Trial 192 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 436, 'num_leaves': 72, 'learning_rate': 0.015387098851021151, 'subsample': 0.8092896431290102, 'colsample_bytree': 0.5947035190956914}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:21,913] Trial 193 finished with value: 0.7344697782372102 and parameters: {'n_estimators': 476, 'num_leaves': 76, 'learning_rate': 0.2594277965788319, 'subsample': 0.8416842236075629, 'colsample_bytree': 0.9742160655239458}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:29,183] Trial 194 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 448, 'num_leaves': 82, 'learning_rate': 0.01308409547124465, 'subsample': 0.855411267019596, 'colsample_bytree': 0.5560915065513172}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:36,496] Trial 195 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 485, 'num_leaves': 80, 'learning_rate': 0.01469989971386253, 'subsample': 0.8279990178085074, 'colsample_bytree': 0.9519485785267198}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:45,523] Trial 196 finished with value: 0.746094283318055 and parameters: {'n_estimators': 491, 'num_leaves': 74, 'learning_rate': 0.011797022850725342, 'subsample': 0.8720763637898898, 'colsample_bytree': 0.5706307024934536}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:52:53,835] Trial 197 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 496, 'num_leaves': 83, 'learning_rate': 0.011997549883985539, 'subsample': 0.8731932425604445, 'colsample_bytree': 0.6052507927044932}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:53:00,998] Trial 198 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 459, 'num_leaves': 88, 'learning_rate': 0.011294585384438904, 'subsample': 0.8473039714260397, 'colsample_bytree': 0.581905227587702}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:53:08,840] Trial 199 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 490, 'num_leaves': 74, 'learning_rate': 0.012558651577570582, 'subsample': 0.8675635208346766, 'colsample_bytree': 0.5687567412233688}. Best is trial 92 with value: 0.746367342162236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ìµœì  LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'n_estimators': 492, 'num_leaves': 88, 'learning_rate': 0.014666949438705097, 'subsample': 0.8766412273868395, 'colsample_bytree': 0.5848756135604947}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # ë³€ê²½ë¨\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # ë³€ê²½ë¨\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)  # ë³€ê²½ë¨\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=200)\n",
    "best_lgb_params = study_lgb.best_params\n",
    "print(f\"ğŸ”¥ ìµœì  LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_lgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 02:54:48,255] A new study created in memory with name: no-name-c2ee9916-5849-441c-ba6f-77a93ad672fe\n",
      "[I 2025-02-18 02:54:55,625] Trial 0 finished with value: 0.745119073160266 and parameters: {'n_estimators': 461, 'max_depth': 9, 'learning_rate': 0.012873750181589297, 'subsample': 0.6184643105349974, 'colsample_bytree': 0.5921870384176341}. Best is trial 0 with value: 0.745119073160266.\n",
      "[I 2025-02-18 02:54:58,060] Trial 1 finished with value: 0.7451775857697334 and parameters: {'n_estimators': 153, 'max_depth': 7, 'learning_rate': 0.0210250488240244, 'subsample': 0.5432974095813292, 'colsample_bytree': 0.7455797821607777}. Best is trial 1 with value: 0.7451775857697334.\n",
      "[I 2025-02-18 02:55:03,106] Trial 2 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 322, 'max_depth': 7, 'learning_rate': 0.027046905842027413, 'subsample': 0.7945150914039159, 'colsample_bytree': 0.6306238463192702}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:07,036] Trial 3 finished with value: 0.7435002242983363 and parameters: {'n_estimators': 268, 'max_depth': 8, 'learning_rate': 0.05483994057265886, 'subsample': 0.6096111610548733, 'colsample_bytree': 0.7241790390317882}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:12,910] Trial 4 finished with value: 0.7428955940005071 and parameters: {'n_estimators': 372, 'max_depth': 9, 'learning_rate': 0.05172611784404157, 'subsample': 0.784025497655918, 'colsample_bytree': 0.7738292981645838}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:16,443] Trial 5 finished with value: 0.7357765598486474 and parameters: {'n_estimators': 175, 'max_depth': 10, 'learning_rate': 0.24716506982203398, 'subsample': 0.8721583349920574, 'colsample_bytree': 0.6089303621396862}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:18,311] Trial 6 finished with value: 0.745041056347643 and parameters: {'n_estimators': 118, 'max_depth': 5, 'learning_rate': 0.2355231550540046, 'subsample': 0.769062902452807, 'colsample_bytree': 0.6177667687494535}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:26,116] Trial 7 finished with value: 0.7411792241227985 and parameters: {'n_estimators': 487, 'max_depth': 8, 'learning_rate': 0.08725996307239292, 'subsample': 0.8329395071796745, 'colsample_bytree': 0.7369654798612159}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:29,606] Trial 8 finished with value: 0.7410817031070196 and parameters: {'n_estimators': 236, 'max_depth': 6, 'learning_rate': 0.2967885166113265, 'subsample': 0.8173244182361457, 'colsample_bytree': 0.511462260929592}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:34,233] Trial 9 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 301, 'max_depth': 3, 'learning_rate': 0.06538076987017612, 'subsample': 0.9550767421355548, 'colsample_bytree': 0.5535703781845907}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:39,796] Trial 10 finished with value: 0.7455286614265374 and parameters: {'n_estimators': 381, 'max_depth': 4, 'learning_rate': 0.027501322871801185, 'subsample': 0.6768856337261048, 'colsample_bytree': 0.8701145608344092}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:45,384] Trial 11 finished with value: 0.745294610988668 and parameters: {'n_estimators': 378, 'max_depth': 4, 'learning_rate': 0.024328128930210927, 'subsample': 0.6826772907089669, 'colsample_bytree': 0.9837028977142386}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:52,394] Trial 12 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 371, 'max_depth': 6, 'learning_rate': 0.029017690767104875, 'subsample': 0.6877922406149006, 'colsample_bytree': 0.9229087582096253}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:55:57,103] Trial 13 finished with value: 0.7438122915488288 and parameters: {'n_estimators': 330, 'max_depth': 3, 'learning_rate': 0.0124503064824266, 'subsample': 0.706760615223476, 'colsample_bytree': 0.8305096131140115}. Best is trial 2 with value: 0.7457237034580952.\n",
      "[I 2025-02-18 02:56:03,992] Trial 14 finished with value: 0.745918745489653 and parameters: {'n_estimators': 422, 'max_depth': 5, 'learning_rate': 0.03569954717501117, 'subsample': 0.9135049355353275, 'colsample_bytree': 0.8704651260444874}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:10,296] Trial 15 finished with value: 0.7440073335803866 and parameters: {'n_estimators': 430, 'max_depth': 5, 'learning_rate': 0.13788247033423512, 'subsample': 0.9962685722842601, 'colsample_bytree': 0.6750435698276422}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:17,507] Trial 16 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 427, 'max_depth': 7, 'learning_rate': 0.03848939929352128, 'subsample': 0.9046736753946922, 'colsample_bytree': 0.8286516588138557}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:22,072] Trial 17 finished with value: 0.7449045269255524 and parameters: {'n_estimators': 225, 'max_depth': 5, 'learning_rate': 0.01718238485220169, 'subsample': 0.9139526742652873, 'colsample_bytree': 0.6810986875545898}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:27,812] Trial 18 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 322, 'max_depth': 6, 'learning_rate': 0.03639007148684383, 'subsample': 0.8656354597666502, 'colsample_bytree': 0.9338265303788742}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:33,637] Trial 19 finished with value: 0.7451385773634218 and parameters: {'n_estimators': 414, 'max_depth': 4, 'learning_rate': 0.09838086552207136, 'subsample': 0.8769758176653767, 'colsample_bytree': 0.9711076150833754}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:43,330] Trial 20 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 497, 'max_depth': 6, 'learning_rate': 0.035385451131590445, 'subsample': 0.9985374769633698, 'colsample_bytree': 0.9070620220062386}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:49,528] Trial 21 finished with value: 0.7451970899728891 and parameters: {'n_estimators': 327, 'max_depth': 7, 'learning_rate': 0.04070066666114993, 'subsample': 0.825637109128091, 'colsample_bytree': 0.9226453003349095}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:56:57,333] Trial 22 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 323, 'max_depth': 8, 'learning_rate': 0.01773961750241935, 'subsample': 0.9422726820940236, 'colsample_bytree': 0.809773580035763}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:57:02,093] Trial 23 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 276, 'max_depth': 5, 'learning_rate': 0.04492815311603364, 'subsample': 0.7418131016399149, 'colsample_bytree': 0.879945248712298}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:57:06,304] Trial 24 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.07028098842411898, 'subsample': 0.7367093039082362, 'colsample_bytree': 0.8766189595433672}. Best is trial 14 with value: 0.745918745489653.\n",
      "[I 2025-02-18 02:57:10,443] Trial 25 finished with value: 0.7462308127401455 and parameters: {'n_estimators': 199, 'max_depth': 6, 'learning_rate': 0.04729185468824702, 'subsample': 0.8577515512057403, 'colsample_bytree': 0.9555586454140826}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:13,548] Trial 26 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 194, 'max_depth': 4, 'learning_rate': 0.13132859303287864, 'subsample': 0.7577868495042372, 'colsample_bytree': 0.9940719356860892}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:17,104] Trial 27 finished with value: 0.7454896530202259 and parameters: {'n_estimators': 207, 'max_depth': 5, 'learning_rate': 0.05050910850244761, 'subsample': 0.9510759124715212, 'colsample_bytree': 0.8707226944711641}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:21,634] Trial 28 finished with value: 0.7445729554719042 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.08124451664499954, 'subsample': 0.6403973392124424, 'colsample_bytree': 0.9533353893170881}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:28,878] Trial 29 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 461, 'max_depth': 4, 'learning_rate': 0.045733938179515064, 'subsample': 0.7307513912241399, 'colsample_bytree': 0.7952812611678256}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:31,271] Trial 30 finished with value: 0.745918745489653 and parameters: {'n_estimators': 115, 'max_depth': 5, 'learning_rate': 0.06228460590697976, 'subsample': 0.8407416965148649, 'colsample_bytree': 0.892259370279719}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:33,318] Trial 31 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 105, 'max_depth': 5, 'learning_rate': 0.06311000460440472, 'subsample': 0.8523892707716534, 'colsample_bytree': 0.8936536920609629}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:35,792] Trial 32 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 130, 'max_depth': 5, 'learning_rate': 0.03150367349361141, 'subsample': 0.9068432160989748, 'colsample_bytree': 0.8401283485720293}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:39,082] Trial 33 finished with value: 0.7432661738604669 and parameters: {'n_estimators': 148, 'max_depth': 6, 'learning_rate': 0.010048647244946786, 'subsample': 0.8120823296794217, 'colsample_bytree': 0.9439121239143096}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:41,756] Trial 34 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 159, 'max_depth': 3, 'learning_rate': 0.10825511306849295, 'subsample': 0.525869694712906, 'colsample_bytree': 0.859248434233797}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:45,746] Trial 35 finished with value: 0.7457041992549395 and parameters: {'n_estimators': 182, 'max_depth': 7, 'learning_rate': 0.046168552850924784, 'subsample': 0.5639641403381493, 'colsample_bytree': 0.7824470989583602}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:49,957] Trial 36 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 287, 'max_depth': 4, 'learning_rate': 0.05887122923405465, 'subsample': 0.7808422123045332, 'colsample_bytree': 0.8960301307828482}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:54,736] Trial 37 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 244, 'max_depth': 5, 'learning_rate': 0.024336331048720704, 'subsample': 0.8497123043970588, 'colsample_bytree': 0.768185645624444}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:57:59,014] Trial 38 finished with value: 0.7461918043338339 and parameters: {'n_estimators': 217, 'max_depth': 6, 'learning_rate': 0.07223092614786719, 'subsample': 0.8029973912970113, 'colsample_bytree': 0.9640039823284605}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:58:03,418] Trial 39 finished with value: 0.7436562579235826 and parameters: {'n_estimators': 208, 'max_depth': 8, 'learning_rate': 0.07860915503619813, 'subsample': 0.8000269218744069, 'colsample_bytree': 0.9993019363701052}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:58:05,877] Trial 40 finished with value: 0.7443194008308791 and parameters: {'n_estimators': 142, 'max_depth': 6, 'learning_rate': 0.16266026122178182, 'subsample': 0.8984774039693072, 'colsample_bytree': 0.9617123174329322}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:58:09,720] Trial 41 finished with value: 0.745567669832849 and parameters: {'n_estimators': 215, 'max_depth': 6, 'learning_rate': 0.05596876442537589, 'subsample': 0.8395448325214724, 'colsample_bytree': 0.9487755837106097}. Best is trial 25 with value: 0.7462308127401455.\n",
      "[I 2025-02-18 02:58:13,189] Trial 42 finished with value: 0.7463868463653918 and parameters: {'n_estimators': 172, 'max_depth': 5, 'learning_rate': 0.046248916226802016, 'subsample': 0.7876352293856931, 'colsample_bytree': 0.9113485663072447}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:17,478] Trial 43 finished with value: 0.7426420393594819 and parameters: {'n_estimators': 169, 'max_depth': 10, 'learning_rate': 0.07232430153337475, 'subsample': 0.8795957027644264, 'colsample_bytree': 0.91240781949831}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:19,948] Trial 44 finished with value: 0.745119073160266 and parameters: {'n_estimators': 103, 'max_depth': 7, 'learning_rate': 0.032554215380502724, 'subsample': 0.7962302392582675, 'colsample_bytree': 0.9677178897434858}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:23,046] Trial 45 finished with value: 0.7451385773634218 and parameters: {'n_estimators': 185, 'max_depth': 5, 'learning_rate': 0.08908657873932295, 'subsample': 0.7671324722833532, 'colsample_bytree': 0.933683462380492}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:26,503] Trial 46 finished with value: 0.7442998966277232 and parameters: {'n_estimators': 133, 'max_depth': 9, 'learning_rate': 0.05036041440211034, 'subsample': 0.9367632438748575, 'colsample_bytree': 0.8457890048832394}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:30,245] Trial 47 finished with value: 0.744943535331864 and parameters: {'n_estimators': 164, 'max_depth': 6, 'learning_rate': 0.020684528340788357, 'subsample': 0.9684805376660843, 'colsample_bytree': 0.8974524872729437}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:33,009] Trial 48 finished with value: 0.7454311404107585 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.06055219716584961, 'subsample': 0.8169052208501122, 'colsample_bytree': 0.7115100719080558}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:36,844] Trial 49 finished with value: 0.745119073160266 and parameters: {'n_estimators': 232, 'max_depth': 4, 'learning_rate': 0.11016324742737793, 'subsample': 0.9252282988657857, 'colsample_bytree': 0.9812806800990662}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:41,528] Trial 50 finished with value: 0.7462308127401455 and parameters: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.04108935839265043, 'subsample': 0.8617439746439712, 'colsample_bytree': 0.9191950985574617}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:46,976] Trial 51 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 256, 'max_depth': 5, 'learning_rate': 0.038887838710785085, 'subsample': 0.8919535752215215, 'colsample_bytree': 0.9242151461563828}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:53,163] Trial 52 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 349, 'max_depth': 6, 'learning_rate': 0.04187672146902617, 'subsample': 0.8597005546620367, 'colsample_bytree': 0.9424080060578808}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:58:56,705] Trial 53 finished with value: 0.745743207661251 and parameters: {'n_estimators': 195, 'max_depth': 5, 'learning_rate': 0.054030945315996524, 'subsample': 0.8268774102905837, 'colsample_bytree': 0.8568672411911852}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:59:01,986] Trial 54 finished with value: 0.745392132004447 and parameters: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.026916967778964376, 'subsample': 0.7860903047223226, 'colsample_bytree': 0.8185285847161989}. Best is trial 42 with value: 0.7463868463653918.\n",
      "[I 2025-02-18 02:59:06,235] Trial 55 finished with value: 0.7464063505685475 and parameters: {'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.06568956129607723, 'subsample': 0.8776657294919856, 'colsample_bytree': 0.9117960993311653}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:10,499] Trial 56 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 227, 'max_depth': 6, 'learning_rate': 0.033005951886948715, 'subsample': 0.8866932327381853, 'colsample_bytree': 0.9128193535113585}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:14,732] Trial 57 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 213, 'max_depth': 7, 'learning_rate': 0.07122520773035089, 'subsample': 0.9759578324423999, 'colsample_bytree': 0.9746200662282116}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:19,672] Trial 58 finished with value: 0.745918745489653 and parameters: {'n_estimators': 239, 'max_depth': 6, 'learning_rate': 0.04873781840605076, 'subsample': 0.9209791957928064, 'colsample_bytree': 0.9577371178502656}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:24,662] Trial 59 finished with value: 0.745294610988668 and parameters: {'n_estimators': 254, 'max_depth': 7, 'learning_rate': 0.04167023393108573, 'subsample': 0.7151883168457864, 'colsample_bytree': 0.9290945030951415}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:28,868] Trial 60 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 195, 'max_depth': 6, 'learning_rate': 0.03686464858082449, 'subsample': 0.8650910965272862, 'colsample_bytree': 0.9853409046158579}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:33,434] Trial 61 finished with value: 0.745119073160266 and parameters: {'n_estimators': 195, 'max_depth': 6, 'learning_rate': 0.02908412021837148, 'subsample': 0.8663154244032732, 'colsample_bytree': 0.9885746736216336}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:37,846] Trial 62 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.035239210028279684, 'subsample': 0.8719175041049161, 'colsample_bytree': 0.9625234446520297}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:42,331] Trial 63 finished with value: 0.7453726278012912 and parameters: {'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.024024261323466804, 'subsample': 0.8019360908525743, 'colsample_bytree': 0.9155104267404752}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:46,881] Trial 64 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.04321923549358256, 'subsample': 0.8410124640245581, 'colsample_bytree': 0.5896591822212306}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 02:59:55,276] Trial 65 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 395, 'max_depth': 5, 'learning_rate': 0.03756018682403228, 'subsample': 0.7752930614330181, 'colsample_bytree': 0.8820132890089061}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:01,152] Trial 66 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 245, 'max_depth': 7, 'learning_rate': 0.05667649778149198, 'subsample': 0.7552907552578293, 'colsample_bytree': 0.9390389235328354}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:07,555] Trial 67 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 295, 'max_depth': 6, 'learning_rate': 0.05198608021286751, 'subsample': 0.8261741329988782, 'colsample_bytree': 0.9777396352180328}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:14,601] Trial 68 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 453, 'max_depth': 5, 'learning_rate': 0.0706883020909919, 'subsample': 0.8998918272104586, 'colsample_bytree': 0.9986587131601558}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:17,507] Trial 69 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 153, 'max_depth': 6, 'learning_rate': 0.06660667397133872, 'subsample': 0.8858156242540955, 'colsample_bytree': 0.9931161887556237}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:22,196] Trial 70 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 270, 'max_depth': 5, 'learning_rate': 0.08598848653268525, 'subsample': 0.8527825301117473, 'colsample_bytree': 0.9515734932516301}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:28,799] Trial 71 finished with value: 0.7447289890971505 and parameters: {'n_estimators': 473, 'max_depth': 4, 'learning_rate': 0.0794536405227831, 'subsample': 0.9189317262755544, 'colsample_bytree': 0.9998184026863688}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:35,358] Trial 72 finished with value: 0.7444949386592811 and parameters: {'n_estimators': 450, 'max_depth': 5, 'learning_rate': 0.10066020289808558, 'subsample': 0.9346642115446165, 'colsample_bytree': 0.9758575858177369}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:42,903] Trial 73 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 445, 'max_depth': 5, 'learning_rate': 0.04688573930760794, 'subsample': 0.9039658788842172, 'colsample_bytree': 0.9083971259585976}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:48,986] Trial 74 finished with value: 0.745392132004447 and parameters: {'n_estimators': 414, 'max_depth': 4, 'learning_rate': 0.0300767797776624, 'subsample': 0.8689519560501953, 'colsample_bytree': 0.9266385412115833}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:00:57,432] Trial 75 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 494, 'max_depth': 5, 'learning_rate': 0.035064745580982416, 'subsample': 0.9594474030203883, 'colsample_bytree': 0.5110421768821372}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:00,987] Trial 76 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 186, 'max_depth': 6, 'learning_rate': 0.06685451821074113, 'subsample': 0.8151527263071664, 'colsample_bytree': 0.9600229338214956}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:08,377] Trial 77 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 480, 'max_depth': 5, 'learning_rate': 0.07264283362796808, 'subsample': 0.8982053837334107, 'colsample_bytree': 0.8830518499794718}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:12,772] Trial 78 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 204, 'max_depth': 6, 'learning_rate': 0.05877183307580951, 'subsample': 0.8460506072400807, 'colsample_bytree': 0.9391261685314785}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:16,693] Trial 79 finished with value: 0.7451775857697334 and parameters: {'n_estimators': 169, 'max_depth': 7, 'learning_rate': 0.026358270516449216, 'subsample': 0.8802782763800996, 'colsample_bytree': 0.9018659901678437}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:20,295] Trial 80 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 227, 'max_depth': 5, 'learning_rate': 0.0921938823927688, 'subsample': 0.8319273904265697, 'colsample_bytree': 0.8681466235147012}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:23,038] Trial 81 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 136, 'max_depth': 5, 'learning_rate': 0.05371151097357411, 'subsample': 0.8599864336237506, 'colsample_bytree': 0.8905971029318316}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:25,521] Trial 82 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 121, 'max_depth': 4, 'learning_rate': 0.060903988755820204, 'subsample': 0.8039707478985566, 'colsample_bytree': 0.9845157331655194}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:30,435] Trial 83 finished with value: 0.7461137875212108 and parameters: {'n_estimators': 283, 'max_depth': 5, 'learning_rate': 0.03893071798554079, 'subsample': 0.909531349902959, 'colsample_bytree': 0.9492990999169579}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:36,495] Trial 84 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 345, 'max_depth': 6, 'learning_rate': 0.039280905348590814, 'subsample': 0.9132630032882245, 'colsample_bytree': 0.9500131887093377}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:41,707] Trial 85 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 282, 'max_depth': 5, 'learning_rate': 0.044805821543795255, 'subsample': 0.9463000001951701, 'colsample_bytree': 0.9655464650266126}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:47,252] Trial 86 finished with value: 0.7450605605507987 and parameters: {'n_estimators': 309, 'max_depth': 4, 'learning_rate': 0.03318662647239727, 'subsample': 0.9291482608647037, 'colsample_bytree': 0.919527194687997}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:51,745] Trial 87 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 261, 'max_depth': 5, 'learning_rate': 0.036956455486581864, 'subsample': 0.892577055541921, 'colsample_bytree': 0.9350399041683748}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:01:57,196] Trial 88 finished with value: 0.7448655185192409 and parameters: {'n_estimators': 246, 'max_depth': 8, 'learning_rate': 0.04720766377592876, 'subsample': 0.8782360350750075, 'colsample_bytree': 0.9718874637608585}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:02:01,451] Trial 89 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 216, 'max_depth': 6, 'learning_rate': 0.051360213579662466, 'subsample': 0.9838228715947506, 'colsample_bytree': 0.9873083377107551}. Best is trial 55 with value: 0.7464063505685475.\n",
      "[I 2025-02-18 03:02:06,481] Trial 90 finished with value: 0.7464258547717033 and parameters: {'n_estimators': 264, 'max_depth': 6, 'learning_rate': 0.030834432692850407, 'subsample': 0.9116953142683085, 'colsample_bytree': 0.9056604893208803}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:11,941] Trial 91 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 264, 'max_depth': 6, 'learning_rate': 0.02220972034926675, 'subsample': 0.9095252590238683, 'colsample_bytree': 0.949694051849597}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:17,790] Trial 92 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.04138975695297763, 'subsample': 0.8576956790802186, 'colsample_bytree': 0.9030139095571035}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:23,280] Trial 93 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 315, 'max_depth': 5, 'learning_rate': 0.028493538189546343, 'subsample': 0.957624346588945, 'colsample_bytree': 0.8541742015652093}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:28,305] Trial 94 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 237, 'max_depth': 6, 'learning_rate': 0.031676524517903405, 'subsample': 0.7875058021530266, 'colsample_bytree': 0.92654154095472}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:32,227] Trial 95 finished with value: 0.7451970899728891 and parameters: {'n_estimators': 187, 'max_depth': 5, 'learning_rate': 0.03519835890452469, 'subsample': 0.8989849209861002, 'colsample_bytree': 0.6491235163132234}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:36,365] Trial 96 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 204, 'max_depth': 7, 'learning_rate': 0.0438534133332986, 'subsample': 0.7667385218734033, 'colsample_bytree': 0.5368156394899659}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:43,660] Trial 97 finished with value: 0.7446314680813715 and parameters: {'n_estimators': 439, 'max_depth': 6, 'learning_rate': 0.07537499946807916, 'subsample': 0.9271106114351311, 'colsample_bytree': 0.9422754372336577}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:49,370] Trial 98 finished with value: 0.7461918043338339 and parameters: {'n_estimators': 275, 'max_depth': 6, 'learning_rate': 0.03955875034243606, 'subsample': 0.9431857622859815, 'colsample_bytree': 0.8687955319997528}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:54,818] Trial 99 finished with value: 0.7450800647539545 and parameters: {'n_estimators': 277, 'max_depth': 7, 'learning_rate': 0.06624584513935837, 'subsample': 0.836556030856271, 'colsample_bytree': 0.746406076344548}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:02:59,705] Trial 100 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 254, 'max_depth': 6, 'learning_rate': 0.03888782435856795, 'subsample': 0.9444950018901327, 'colsample_bytree': 0.9152233620091975}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:04,399] Trial 101 finished with value: 0.746269821146457 and parameters: {'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.04920810240707508, 'subsample': 0.9165621247797283, 'colsample_bytree': 0.8691132086413657}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:08,496] Trial 102 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 223, 'max_depth': 6, 'learning_rate': 0.048215880521538156, 'subsample': 0.9147833672541881, 'colsample_bytree': 0.8330708101328176}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:12,368] Trial 103 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 212, 'max_depth': 6, 'learning_rate': 0.04842710275995667, 'subsample': 0.9692870363638645, 'colsample_bytree': 0.8355496048853681}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:16,938] Trial 104 finished with value: 0.7460552749117435 and parameters: {'n_estimators': 231, 'max_depth': 6, 'learning_rate': 0.05465225371507253, 'subsample': 0.9355819217047509, 'colsample_bytree': 0.8683767887341196}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:21,662] Trial 105 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 231, 'max_depth': 6, 'learning_rate': 0.054817958523843774, 'subsample': 0.9382084510555768, 'colsample_bytree': 0.8534867045902577}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:26,096] Trial 106 finished with value: 0.7449240311287082 and parameters: {'n_estimators': 222, 'max_depth': 7, 'learning_rate': 0.04945227440043291, 'subsample': 0.9875313813374926, 'colsample_bytree': 0.8671368609610446}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:30,677] Trial 107 finished with value: 0.7462113085369897 and parameters: {'n_estimators': 243, 'max_depth': 6, 'learning_rate': 0.042628032114923914, 'subsample': 0.9187614271357448, 'colsample_bytree': 0.8855490404741979}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:35,838] Trial 108 finished with value: 0.7455091572233816 and parameters: {'n_estimators': 243, 'max_depth': 6, 'learning_rate': 0.043170525298451036, 'subsample': 0.954440830531482, 'colsample_bytree': 0.8069195867495316}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:40,544] Trial 109 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 269, 'max_depth': 6, 'learning_rate': 0.05682145215193514, 'subsample': 0.9282803176238859, 'colsample_bytree': 0.8856070765014985}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:45,502] Trial 110 finished with value: 0.7452751067855122 and parameters: {'n_estimators': 292, 'max_depth': 6, 'learning_rate': 0.06307786004173245, 'subsample': 0.8886681796273925, 'colsample_bytree': 0.8728276411080643}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:50,879] Trial 111 finished with value: 0.7459577538959645 and parameters: {'n_estimators': 248, 'max_depth': 6, 'learning_rate': 0.040349696374517574, 'subsample': 0.9160986273665499, 'colsample_bytree': 0.8904751906255276}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:03:55,335] Trial 112 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 235, 'max_depth': 6, 'learning_rate': 0.04476135053469451, 'subsample': 0.9071014859886722, 'colsample_bytree': 0.8282582108773029}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:00,028] Trial 113 finished with value: 0.7460747791148993 and parameters: {'n_estimators': 259, 'max_depth': 6, 'learning_rate': 0.053357190257632504, 'subsample': 0.9192276823481225, 'colsample_bytree': 0.8462958415039377}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:05,463] Trial 114 finished with value: 0.744767997503462 and parameters: {'n_estimators': 262, 'max_depth': 7, 'learning_rate': 0.05318898832972798, 'subsample': 0.9364597302925485, 'colsample_bytree': 0.863540408566021}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:10,554] Trial 115 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 254, 'max_depth': 6, 'learning_rate': 0.05807127719572926, 'subsample': 0.9634377760509487, 'colsample_bytree': 0.8777916980987609}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:16,092] Trial 116 finished with value: 0.7417643502174719 and parameters: {'n_estimators': 272, 'max_depth': 6, 'learning_rate': 0.2151079886243966, 'subsample': 0.9457547488635291, 'colsample_bytree': 0.8996556675433074}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:23,617] Trial 117 finished with value: 0.745743207661251 and parameters: {'n_estimators': 284, 'max_depth': 6, 'learning_rate': 0.03059163336349928, 'subsample': 0.6641791399285131, 'colsample_bytree': 0.9078436518139028}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:29,114] Trial 118 finished with value: 0.746094283318055 and parameters: {'n_estimators': 232, 'max_depth': 6, 'learning_rate': 0.040141796881801195, 'subsample': 0.8832302655457143, 'colsample_bytree': 0.7242075780029453}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:35,560] Trial 119 finished with value: 0.7458407286770299 and parameters: {'n_estimators': 279, 'max_depth': 7, 'learning_rate': 0.03442322775533612, 'subsample': 0.8697591465991517, 'colsample_bytree': 0.6872398583092454}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:39,961] Trial 120 finished with value: 0.7456066782391605 and parameters: {'n_estimators': 203, 'max_depth': 5, 'learning_rate': 0.04080841901208715, 'subsample': 0.605922479408082, 'colsample_bytree': 0.732415582752366}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:44,255] Trial 121 finished with value: 0.746094283318055 and parameters: {'n_estimators': 233, 'max_depth': 6, 'learning_rate': 0.05057414640358783, 'subsample': 0.8820247848186763, 'colsample_bytree': 0.7184644594270866}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:48,464] Trial 122 finished with value: 0.745294610988668 and parameters: {'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.03780705654590967, 'subsample': 0.8851224447164168, 'colsample_bytree': 0.69720621154561}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:53,709] Trial 123 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 249, 'max_depth': 6, 'learning_rate': 0.0443694230935604, 'subsample': 0.8949521789260299, 'colsample_bytree': 0.7632225420458413}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:04:58,708] Trial 124 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 261, 'max_depth': 6, 'learning_rate': 0.041872388816983616, 'subsample': 0.8770117154079748, 'colsample_bytree': 0.7209732115828313}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:02,701] Trial 125 finished with value: 0.745567669832849 and parameters: {'n_estimators': 210, 'max_depth': 6, 'learning_rate': 0.04518024689044204, 'subsample': 0.9212843686455723, 'colsample_bytree': 0.7111800453102981}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:06,957] Trial 126 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 237, 'max_depth': 5, 'learning_rate': 0.05001877092065551, 'subsample': 0.7447651680811952, 'colsample_bytree': 0.7557691352955951}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:13,057] Trial 127 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 304, 'max_depth': 7, 'learning_rate': 0.06167491941708975, 'subsample': 0.8525487256385613, 'colsample_bytree': 0.739688381103072}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:16,483] Trial 128 finished with value: 0.7451775857697334 and parameters: {'n_estimators': 173, 'max_depth': 6, 'learning_rate': 0.03854974401453441, 'subsample': 0.9041678827070332, 'colsample_bytree': 0.6604862436711764}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:19,711] Trial 129 finished with value: 0.7453726278012912 and parameters: {'n_estimators': 161, 'max_depth': 6, 'learning_rate': 0.032866850115808345, 'subsample': 0.8732404466633956, 'colsample_bytree': 0.8485236815440124}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:23,725] Trial 130 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 189, 'max_depth': 6, 'learning_rate': 0.05109520333717756, 'subsample': 0.8209405121477655, 'colsample_bytree': 0.9213166645061073}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:28,652] Trial 131 finished with value: 0.7449045269255524 and parameters: {'n_estimators': 232, 'max_depth': 6, 'learning_rate': 0.05485576927751831, 'subsample': 0.9279255500019916, 'colsample_bytree': 0.8951842469736518}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:33,143] Trial 132 finished with value: 0.745041056347643 and parameters: {'n_estimators': 226, 'max_depth': 6, 'learning_rate': 0.0464683116528717, 'subsample': 0.9096716354518821, 'colsample_bytree': 0.8799604301053346}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:38,084] Trial 133 finished with value: 0.7450020479413314 and parameters: {'n_estimators': 243, 'max_depth': 6, 'learning_rate': 0.06699926769715295, 'subsample': 0.8899362702304902, 'colsample_bytree': 0.7796849920705385}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:43,289] Trial 134 finished with value: 0.746094283318055 and parameters: {'n_estimators': 254, 'max_depth': 5, 'learning_rate': 0.05203790570172148, 'subsample': 0.9364624339768391, 'colsample_bytree': 0.7079148932968456}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:48,046] Trial 135 finished with value: 0.746094283318055 and parameters: {'n_estimators': 251, 'max_depth': 5, 'learning_rate': 0.0365246628936086, 'subsample': 0.7233904656078799, 'colsample_bytree': 0.7257011321258031}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:53,219] Trial 136 finished with value: 0.7456846950517837 and parameters: {'n_estimators': 268, 'max_depth': 5, 'learning_rate': 0.03640249430497461, 'subsample': 0.8635287018022633, 'colsample_bytree': 0.7159670716603408}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:05:58,619] Trial 137 finished with value: 0.7463088295527687 and parameters: {'n_estimators': 252, 'max_depth': 5, 'learning_rate': 0.04073743563458141, 'subsample': 0.7176539558035666, 'colsample_bytree': 0.695772023874828}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:02,890] Trial 138 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 215, 'max_depth': 5, 'learning_rate': 0.04066599435934993, 'subsample': 0.7336028289106895, 'colsample_bytree': 0.6943573475517906}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:07,740] Trial 139 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 239, 'max_depth': 5, 'learning_rate': 0.026517486569594226, 'subsample': 0.7098264140285484, 'colsample_bytree': 0.6794845434719985}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:13,526] Trial 140 finished with value: 0.7461137875212108 and parameters: {'n_estimators': 297, 'max_depth': 5, 'learning_rate': 0.0427776033568168, 'subsample': 0.8066851969698147, 'colsample_bytree': 0.6654992354098981}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:19,115] Trial 141 finished with value: 0.7463088295527687 and parameters: {'n_estimators': 291, 'max_depth': 5, 'learning_rate': 0.04736917763216715, 'subsample': 0.7927777585046428, 'colsample_bytree': 0.6649181630200076}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:24,761] Trial 142 finished with value: 0.7458212244738741 and parameters: {'n_estimators': 306, 'max_depth': 5, 'learning_rate': 0.046918645550618726, 'subsample': 0.774490927010482, 'colsample_bytree': 0.6513576205866299}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:31,160] Trial 143 finished with value: 0.7463283337559244 and parameters: {'n_estimators': 316, 'max_depth': 5, 'learning_rate': 0.04253936290586642, 'subsample': 0.6921270893718114, 'colsample_bytree': 0.6982778073617565}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:36,708] Trial 144 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 290, 'max_depth': 5, 'learning_rate': 0.04151267883262674, 'subsample': 0.7002195519278103, 'colsample_bytree': 0.6504228209568964}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:42,747] Trial 145 finished with value: 0.7461527959275224 and parameters: {'n_estimators': 320, 'max_depth': 5, 'learning_rate': 0.03333141645567138, 'subsample': 0.6927641402259217, 'colsample_bytree': 0.6731463265937023}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:49,258] Trial 146 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 315, 'max_depth': 5, 'learning_rate': 0.04302630395800317, 'subsample': 0.6845671038676916, 'colsample_bytree': 0.6282493860472144}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:06:55,386] Trial 147 finished with value: 0.7456456866454721 and parameters: {'n_estimators': 334, 'max_depth': 5, 'learning_rate': 0.033804881293340236, 'subsample': 0.6511881193498822, 'colsample_bytree': 0.6668356989322083}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:01,817] Trial 148 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 336, 'max_depth': 4, 'learning_rate': 0.029928085706279493, 'subsample': 0.6761988892843853, 'colsample_bytree': 0.6679601687156569}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:07,697] Trial 149 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 296, 'max_depth': 5, 'learning_rate': 0.03238253560171218, 'subsample': 0.6947921775306695, 'colsample_bytree': 0.600636711789942}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:12,929] Trial 150 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 309, 'max_depth': 4, 'learning_rate': 0.0381666934578194, 'subsample': 0.7180778310645173, 'colsample_bytree': 0.6346552509043215}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:18,758] Trial 151 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 280, 'max_depth': 5, 'learning_rate': 0.03905179510142509, 'subsample': 0.7617826456239927, 'colsample_bytree': 0.6921738848356398}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:24,779] Trial 152 finished with value: 0.7461918043338339 and parameters: {'n_estimators': 298, 'max_depth': 5, 'learning_rate': 0.03540868835861291, 'subsample': 0.8063644843043621, 'colsample_bytree': 0.7021278002605625}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:31,094] Trial 153 finished with value: 0.7458017202707183 and parameters: {'n_estimators': 316, 'max_depth': 5, 'learning_rate': 0.0360008626308299, 'subsample': 0.8111398033200109, 'colsample_bytree': 0.702167746297847}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:37,559] Trial 154 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 324, 'max_depth': 5, 'learning_rate': 0.027669417293137252, 'subsample': 0.7932337299924266, 'colsample_bytree': 0.6747354652741275}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:43,253] Trial 155 finished with value: 0.7460552749117435 and parameters: {'n_estimators': 303, 'max_depth': 5, 'learning_rate': 0.043680909030522755, 'subsample': 0.6717307811898042, 'colsample_bytree': 0.6803204448340102}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:49,301] Trial 156 finished with value: 0.746094283318055 and parameters: {'n_estimators': 297, 'max_depth': 5, 'learning_rate': 0.03442236916742322, 'subsample': 0.7873467106695484, 'colsample_bytree': 0.6608321285961387}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:07:55,327] Trial 157 finished with value: 0.7457237034580952 and parameters: {'n_estimators': 286, 'max_depth': 5, 'learning_rate': 0.08317259257038827, 'subsample': 0.7957632629013527, 'colsample_bytree': 0.9356461586675304}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:00,985] Trial 158 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 346, 'max_depth': 3, 'learning_rate': 0.04583892025415016, 'subsample': 0.8139549557538215, 'colsample_bytree': 0.6301832591374754}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:08,131] Trial 159 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 272, 'max_depth': 5, 'learning_rate': 0.03047590990804028, 'subsample': 0.8461679996337027, 'colsample_bytree': 0.6383821882863305}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:15,481] Trial 160 finished with value: 0.7459577538959645 and parameters: {'n_estimators': 315, 'max_depth': 5, 'learning_rate': 0.047847652129478994, 'subsample': 0.8048841613269194, 'colsample_bytree': 0.6841332333479321}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:23,280] Trial 161 finished with value: 0.7461918043338339 and parameters: {'n_estimators': 325, 'max_depth': 5, 'learning_rate': 0.04019010505536317, 'subsample': 0.6975956652837773, 'colsample_bytree': 0.732244765037531}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:30,654] Trial 162 finished with value: 0.7456651908486279 and parameters: {'n_estimators': 360, 'max_depth': 5, 'learning_rate': 0.0414247278717094, 'subsample': 0.6950077491583173, 'colsample_bytree': 0.9556341261217332}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:37,633] Trial 163 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 339, 'max_depth': 5, 'learning_rate': 0.037662263555100224, 'subsample': 0.7039382649144446, 'colsample_bytree': 0.7060104373966265}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:43,745] Trial 164 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 327, 'max_depth': 5, 'learning_rate': 0.04307688589397182, 'subsample': 0.7506371397868236, 'colsample_bytree': 0.9200276993416108}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:49,056] Trial 165 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 293, 'max_depth': 4, 'learning_rate': 0.034324271123806226, 'subsample': 0.6901927815304487, 'colsample_bytree': 0.6698054695282727}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:08:54,786] Trial 166 finished with value: 0.7451970899728891 and parameters: {'n_estimators': 275, 'max_depth': 5, 'learning_rate': 0.03172331157845591, 'subsample': 0.8343232027362621, 'colsample_bytree': 0.9063366272461038}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:00,494] Trial 167 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.0482268576264157, 'subsample': 0.7264219778969168, 'colsample_bytree': 0.6941671882210374}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:06,503] Trial 168 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 310, 'max_depth': 5, 'learning_rate': 0.04017938374409102, 'subsample': 0.6599302305313709, 'colsample_bytree': 0.9273343369108514}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:13,026] Trial 169 finished with value: 0.7461723001306781 and parameters: {'n_estimators': 326, 'max_depth': 5, 'learning_rate': 0.03614680275898724, 'subsample': 0.7756619128448787, 'colsample_bytree': 0.6590188144539774}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:19,365] Trial 170 finished with value: 0.7455871740360047 and parameters: {'n_estimators': 321, 'max_depth': 5, 'learning_rate': 0.024282999166182456, 'subsample': 0.7698218293986621, 'colsample_bytree': 0.7333517944079593}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:25,714] Trial 171 finished with value: 0.7461527959275224 and parameters: {'n_estimators': 321, 'max_depth': 5, 'learning_rate': 0.038178464510945496, 'subsample': 0.7852808342205148, 'colsample_bytree': 0.6459803211166503}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:32,259] Trial 172 finished with value: 0.7461137875212108 and parameters: {'n_estimators': 329, 'max_depth': 5, 'learning_rate': 0.036108514186586366, 'subsample': 0.7837979358169876, 'colsample_bytree': 0.6532545115249848}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:38,582] Trial 173 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 325, 'max_depth': 5, 'learning_rate': 0.03846676829355114, 'subsample': 0.7575145055299433, 'colsample_bytree': 0.6220927311302313}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:44,352] Trial 174 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 285, 'max_depth': 5, 'learning_rate': 0.04493820251982091, 'subsample': 0.7757426355982755, 'colsample_bytree': 0.964899963846896}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:48,380] Trial 175 finished with value: 0.7442998966277232 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.015032526532245372, 'subsample': 0.7943002993706977, 'colsample_bytree': 0.945595850676854}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:09:57,497] Trial 176 finished with value: 0.7437147705330499 and parameters: {'n_estimators': 321, 'max_depth': 10, 'learning_rate': 0.03249336664327189, 'subsample': 0.7405205403991002, 'colsample_bytree': 0.9121943922546346}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:04,376] Trial 177 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 344, 'max_depth': 5, 'learning_rate': 0.03539465959687878, 'subsample': 0.5041410710143976, 'colsample_bytree': 0.6791597908118785}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:09,392] Trial 178 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 264, 'max_depth': 5, 'learning_rate': 0.07582901257654696, 'subsample': 0.784318532203148, 'colsample_bytree': 0.6440848447991493}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:16,086] Trial 179 finished with value: 0.7457822160675626 and parameters: {'n_estimators': 337, 'max_depth': 5, 'learning_rate': 0.03954577718398655, 'subsample': 0.7170193973196979, 'colsample_bytree': 0.6876904423386044}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:19,666] Trial 180 finished with value: 0.7459967623022762 and parameters: {'n_estimators': 181, 'max_depth': 5, 'learning_rate': 0.04634425586885581, 'subsample': 0.7078639797717683, 'colsample_bytree': 0.6549827413114141}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:25,538] Trial 181 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 311, 'max_depth': 5, 'learning_rate': 0.04269391962221151, 'subsample': 0.8025057798559315, 'colsample_bytree': 0.6687255518516475}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:31,675] Trial 182 finished with value: 0.7459382496928088 and parameters: {'n_estimators': 301, 'max_depth': 5, 'learning_rate': 0.03734651755751543, 'subsample': 0.8197924117695092, 'colsample_bytree': 0.7009272351637938}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:37,328] Trial 183 finished with value: 0.7459772580991204 and parameters: {'n_estimators': 291, 'max_depth': 5, 'learning_rate': 0.040913449226278056, 'subsample': 0.8069170785625228, 'colsample_bytree': 0.8874479561178816}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:43,726] Trial 184 finished with value: 0.7461332917243666 and parameters: {'n_estimators': 281, 'max_depth': 5, 'learning_rate': 0.043515198284910496, 'subsample': 0.6820245356469087, 'colsample_bytree': 0.6582038559728695}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:49,261] Trial 185 finished with value: 0.745918745489653 and parameters: {'n_estimators': 274, 'max_depth': 5, 'learning_rate': 0.03419177024610523, 'subsample': 0.6847113374802496, 'colsample_bytree': 0.6433297628925237}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:54,657] Trial 186 finished with value: 0.7456261824423163 and parameters: {'n_estimators': 266, 'max_depth': 6, 'learning_rate': 0.04547742186154225, 'subsample': 0.6724061452143054, 'colsample_bytree': 0.896277973475324}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:10:59,586] Trial 187 finished with value: 0.7460357707085877 and parameters: {'n_estimators': 278, 'max_depth': 4, 'learning_rate': 0.0496856524802324, 'subsample': 0.6887070618213613, 'colsample_bytree': 0.6619177034594622}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:06,287] Trial 188 finished with value: 0.746016266505432 and parameters: {'n_estimators': 320, 'max_depth': 5, 'learning_rate': 0.02908396370949423, 'subsample': 0.6325236470787072, 'colsample_bytree': 0.6787674183186106}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:12,943] Trial 189 finished with value: 0.745392132004447 and parameters: {'n_estimators': 331, 'max_depth': 6, 'learning_rate': 0.038809772660685554, 'subsample': 0.7767273337220887, 'colsample_bytree': 0.6114713318480772}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:17,586] Trial 190 finished with value: 0.7452360983792007 and parameters: {'n_estimators': 211, 'max_depth': 5, 'learning_rate': 0.036321328390963296, 'subsample': 0.699604200192254, 'colsample_bytree': 0.9073822266372304}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:23,457] Trial 191 finished with value: 0.7458992412864972 and parameters: {'n_estimators': 297, 'max_depth': 5, 'learning_rate': 0.04153746974931767, 'subsample': 0.7920520387456998, 'colsample_bytree': 0.660432709151459}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:29,075] Trial 192 finished with value: 0.7454701488170701 and parameters: {'n_estimators': 306, 'max_depth': 5, 'learning_rate': 0.042592136333580104, 'subsample': 0.7815635964285551, 'colsample_bytree': 0.686932071529291}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:35,001] Trial 193 finished with value: 0.7463478379590802 and parameters: {'n_estimators': 288, 'max_depth': 5, 'learning_rate': 0.043377138081276356, 'subsample': 0.8233685260087202, 'colsample_bytree': 0.6723234706036954}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:40,248] Trial 194 finished with value: 0.7460552749117435 and parameters: {'n_estimators': 284, 'max_depth': 5, 'learning_rate': 0.04857564124171163, 'subsample': 0.6805309211316485, 'colsample_bytree': 0.6750710054152622}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:45,768] Trial 195 finished with value: 0.745567669832849 and parameters: {'n_estimators': 257, 'max_depth': 6, 'learning_rate': 0.03839843734250678, 'subsample': 0.8311358275558274, 'colsample_bytree': 0.6450068503735588}. Best is trial 90 with value: 0.7464258547717033.\n",
      "[I 2025-02-18 03:11:53,062] Trial 196 finished with value: 0.7465623841937937 and parameters: {'n_estimators': 359, 'max_depth': 5, 'learning_rate': 0.045093620098282834, 'subsample': 0.6669970717173888, 'colsample_bytree': 0.934119080218835}. Best is trial 196 with value: 0.7465623841937937.\n",
      "[I 2025-02-18 03:11:59,287] Trial 197 finished with value: 0.745743207661251 and parameters: {'n_estimators': 356, 'max_depth': 5, 'learning_rate': 0.0579565737118503, 'subsample': 0.6631315488600884, 'colsample_bytree': 0.9229232517398936}. Best is trial 196 with value: 0.7465623841937937.\n",
      "[I 2025-02-18 03:12:07,032] Trial 198 finished with value: 0.745567669832849 and parameters: {'n_estimators': 391, 'max_depth': 6, 'learning_rate': 0.05171982964480489, 'subsample': 0.6685114326378423, 'colsample_bytree': 0.9338560651711239}. Best is trial 196 with value: 0.7465623841937937.\n",
      "[I 2025-02-18 03:12:11,177] Trial 199 finished with value: 0.7457627118644068 and parameters: {'n_estimators': 222, 'max_depth': 5, 'learning_rate': 0.045740957339408604, 'subsample': 0.766483831771843, 'colsample_bytree': 0.6948319379344035}. Best is trial 196 with value: 0.7465623841937937.\n",
      "[I 2025-02-18 03:12:11,179] A new study created in memory with name: no-name-a8fda882-390f-44d0-ad9a-3e4e21b05ea6\n",
      "[W 2025-02-18 03:12:11,187] Trial 0 failed with parameters: {'n_estimators': 242, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'auto'} because of the following error: InvalidParameterError(\"The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pung\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pung\\AppData\\Local\\Temp\\ipykernel_9188\\443491682.py\", line 34, in objective_rf\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\pung\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pung\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pung\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "[W 2025-02-18 03:12:11,199] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ìµœì  XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'n_estimators': 359, 'max_depth': 5, 'learning_rate': 0.045093620098282834, 'subsample': 0.6669970717173888, 'colsample_bytree': 0.934119080218835}\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Optuna ìŠ¤í„°ë”” ìƒì„±\u001b[39;00m\n\u001b[0;32m     38\u001b[0m study_rf \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m study_rf\u001b[38;5;241m.\u001b[39moptimize(objective_rf, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     40\u001b[0m best_rf_params \u001b[38;5;241m=\u001b[39m study_rf\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”¥ ìµœì  Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rf_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mobjective_rf\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     25\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     31\u001b[0m }\n\u001b[0;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mscore(X_val, y_val)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    669\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    670\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # ë³€ê²½ë¨\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # ë³€ê²½ë¨\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)  # ë³€ê²½ë¨\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=200)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "print(f\"ğŸ”¥ ìµœì  XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_xgb_params}\")\n",
    "\n",
    "# ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "# Optuna ìŠ¤í„°ë”” ìƒì„±\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=200)\n",
    "best_rf_params = study_rf.best_params\n",
    "print(f\"ğŸ”¥ ìµœì  Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 03:15:37,585] A new study created in memory with name: no-name-748e1273-70cd-4b9c-a12a-a4a2a5c6720c\n",
      "[I 2025-02-18 03:16:33,558] Trial 0 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 136, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 0 with value: 0.7416473249985371.\n",
      "[I 2025-02-18 03:18:30,335] Trial 1 finished with value: 0.7442608882214117 and parameters: {'n_estimators': 163, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:19:26,608] Trial 2 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 135, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:20:13,793] Trial 3 finished with value: 0.7422324510932106 and parameters: {'n_estimators': 328, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:22:07,965] Trial 4 finished with value: 0.7435587369078036 and parameters: {'n_estimators': 220, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:23:37,053] Trial 5 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 285, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:23:55,094] Trial 6 finished with value: 0.7420569132648086 and parameters: {'n_estimators': 138, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:24:53,017] Trial 7 finished with value: 0.7421349300774317 and parameters: {'n_estimators': 454, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:25:13,546] Trial 8 finished with value: 0.7428370813910398 and parameters: {'n_estimators': 133, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7442608882214117.\n",
      "[I 2025-02-18 03:27:21,501] Trial 9 finished with value: 0.7442998966277232 and parameters: {'n_estimators': 173, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 9 with value: 0.7442998966277232.\n",
      "[I 2025-02-18 03:28:00,721] Trial 10 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 407, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 9 with value: 0.7442998966277232.\n",
      "[I 2025-02-18 03:30:21,881] Trial 11 finished with value: 0.7439488209709192 and parameters: {'n_estimators': 226, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 9 with value: 0.7442998966277232.\n",
      "[I 2025-02-18 03:33:09,872] Trial 12 finished with value: 0.7444364260498137 and parameters: {'n_estimators': 215, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:36:09,631] Trial 13 finished with value: 0.7439098125646076 and parameters: {'n_estimators': 224, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:39:13,614] Trial 14 finished with value: 0.7440658461898539 and parameters: {'n_estimators': 298, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:39:33,731] Trial 15 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 187, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:41:50,832] Trial 16 finished with value: 0.7435587369078036 and parameters: {'n_estimators': 266, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:45:59,059] Trial 17 finished with value: 0.7444364260498137 and parameters: {'n_estimators': 349, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:46:43,523] Trial 18 finished with value: 0.7418033586237834 and parameters: {'n_estimators': 383, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.7444364260498137.\n",
      "[I 2025-02-18 03:47:20,432] Trial 19 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 358, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.7444364260498137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ìµœì  Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'n_estimators': 215, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),  # ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "# Optuna ìŠ¤í„°ë”” ìƒì„±\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=20)\n",
    "best_rf_params = study_rf.best_params\n",
    "print(f\"ğŸ”¥ ìµœì  Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35321, number of negative: 101399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 697\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258346 -> initscore=-1.054586\n",
      "[LightGBM] [Info] Start training from score -1.054586\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35321, number of negative: 101399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 696\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258346 -> initscore=-1.054586\n",
      "[LightGBM] [Info] Start training from score -1.054586\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35322, number of negative: 101398\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258353 -> initscore=-1.054547\n",
      "[LightGBM] [Info] Start training from score -1.054547\n",
      "âœ… Stacking ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "Confusion Matrix\n",
      "[[35177  2848]\n",
      " [10334  2912]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84     38025\n",
      "           1       0.51      0.22      0.31     13246\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.64      0.57      0.57     51271\n",
      "weighted avg       0.70      0.74      0.70     51271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê°œë³„ ëª¨ë¸ ì •ì˜\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=215,\n",
    "    max_depth=7,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=359,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.045093620098282834,\n",
    "    subsample=0.6669970717173888,\n",
    "    colsample_bytree=0.934119080218835,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=492,\n",
    "    num_leaves=88,\n",
    "    learning_rate=0.014666949438705097,\n",
    "    subsample=0.8766412273868395,\n",
    "    colsample_bytree=0.5848756135604947,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stacking ì•™ìƒë¸” ì •ì˜\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "stacking_model.fit(X_train, y_train)\n",
    "print(\"âœ… Stacking ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡\n",
    "y_pred = stacking_model.predict(X_val)\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ í™•ì¸\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6963029503822327\n",
      "Epoch 10, Loss: 0.5769034028053284\n",
      "Epoch 20, Loss: 0.5450228452682495\n",
      "Epoch 30, Loss: 0.5260497331619263\n",
      "Epoch 40, Loss: 0.5124301314353943\n",
      "âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch ëª¨ë¸ ì •ì˜\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.7029680013656616, Validation Loss: 0.6860185861587524\n",
      "Epoch 10, Train Loss: 0.5763489007949829, Validation Loss: 0.5762265920639038\n",
      "Epoch 20, Train Loss: 0.5395788550376892, Validation Loss: 0.5381858944892883\n",
      "Epoch 30, Train Loss: 0.5211194157600403, Validation Loss: 0.5207911133766174\n",
      "Epoch 40, Train Loss: 0.5083341002464294, Validation Loss: 0.5086693167686462\n",
      "Epoch 50, Train Loss: 0.5013975501060486, Validation Loss: 0.502371609210968\n",
      "Epoch 60, Train Loss: 0.4975058436393738, Validation Loss: 0.49911773204803467\n",
      "Epoch 70, Train Loss: 0.4954114258289337, Validation Loss: 0.4975021183490753\n",
      "Epoch 80, Train Loss: 0.4937605857849121, Validation Loss: 0.4961315393447876\n",
      "Epoch 90, Train Loss: 0.4924192428588867, Validation Loss: 0.49504372477531433\n",
      "Epoch 100, Train Loss: 0.4913056492805481, Validation Loss: 0.49416810274124146\n",
      "Epoch 110, Train Loss: 0.49032527208328247, Validation Loss: 0.4935409426689148\n",
      "Epoch 120, Train Loss: 0.48945632576942444, Validation Loss: 0.49306777119636536\n",
      "Epoch 130, Train Loss: 0.48867303133010864, Validation Loss: 0.49269533157348633\n",
      "Epoch 140, Train Loss: 0.48794153332710266, Validation Loss: 0.4923964738845825\n",
      "Epoch 150, Train Loss: 0.48724034428596497, Validation Loss: 0.4921836256980896\n",
      "Epoch 160, Train Loss: 0.4865817725658417, Validation Loss: 0.492067813873291\n",
      "Epoch 170, Train Loss: 0.48594579100608826, Validation Loss: 0.4919586181640625\n",
      "Epoch 180, Train Loss: 0.48531341552734375, Validation Loss: 0.4919484853744507\n",
      "Early stopping at epoch 182\n",
      "âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch ëª¨ë¸ ì •ì˜\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# í•™ìŠµ ê³¼ì •ì—ì„œ ê²€ì¦ ì†ì‹¤ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # ì¡°ê¸° ì¢…ë£Œì˜ patience ì„¤ì •\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ê²€ì¦ ë°ì´í„°ì—ì„œ ì†ì‹¤ ê³„ì‚°\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "    # ì¡°ê¸° ì¢…ë£Œ\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(\"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ 'baseline_submit2.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Stacking ëª¨ë¸ + ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ ê²°í•©\n",
    "stacking_preds = stacking_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# ë”¥ëŸ¬ë‹ ì˜ˆì¸¡\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "dl_preds = model(X_test_tensor).cpu().detach().numpy().flatten()\n",
    "\n",
    "# í‰ê·  Ensemble\n",
    "final_preds = (stacking_preds + dl_preds) / 2\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = final_preds\n",
    "sample_submission.to_csv('../submission/baseline_submit2.csv', index=False)\n",
    "print(\"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ 'baseline_submit2.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
