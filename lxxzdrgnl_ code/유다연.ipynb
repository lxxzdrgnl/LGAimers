{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\").drop(columns=['ID'])\n",
    "test = pd.read_csv(\"../data/test.csv\").drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"시술 시기 코드\",\n",
    "    \"시술 당시 나이\",\n",
    "    \"시술 유형\",\n",
    "    \"특정 시술 유형\",\n",
    "    \"배란 자극 여부\",\n",
    "    \"배란 유도 유형\",\n",
    "    \"단일 배아 이식 여부\",\n",
    "    \"착상 전 유전 검사 사용 여부\",\n",
    "    \"착상 전 유전 진단 사용 여부\",\n",
    "    \"남성 주 불임 원인\",\n",
    "    \"남성 부 불임 원인\",\n",
    "    \"여성 주 불임 원인\",\n",
    "    \"여성 부 불임 원인\",\n",
    "    \"부부 주 불임 원인\",\n",
    "    \"부부 부 불임 원인\",\n",
    "    \"불명확 불임 원인\",\n",
    "    \"불임 원인 - 난관 질환\",\n",
    "    \"불임 원인 - 남성 요인\",\n",
    "    \"불임 원인 - 배란 장애\",\n",
    "    \"불임 원인 - 여성 요인\",\n",
    "    \"불임 원인 - 자궁경부 문제\",\n",
    "    \"불임 원인 - 자궁내막증\",\n",
    "    \"불임 원인 - 정자 농도\",\n",
    "    \"불임 원인 - 정자 면역학적 요인\",\n",
    "    \"불임 원인 - 정자 운동성\",\n",
    "    \"불임 원인 - 정자 형태\",\n",
    "    \"배아 생성 주요 이유\",\n",
    "    \"총 시술 횟수\",\n",
    "    \"클리닉 내 총 시술 횟수\",\n",
    "    \"IVF 시술 횟수\",\n",
    "    \"DI 시술 횟수\",\n",
    "    \"총 임신 횟수\",\n",
    "    \"IVF 임신 횟수\",\n",
    "    \"DI 임신 횟수\",\n",
    "    \"총 출산 횟수\",\n",
    "    \"IVF 출산 횟수\",\n",
    "    \"DI 출산 횟수\",\n",
    "    \"난자 출처\",\n",
    "    \"정자 출처\",\n",
    "    \"난자 기증자 나이\",\n",
    "    \"정자 기증자 나이\",\n",
    "    \"동결 배아 사용 여부\",\n",
    "    \"신선 배아 사용 여부\",\n",
    "    \"기증 배아 사용 여부\",\n",
    "    \"대리모 여부\",\n",
    "    \"PGD 시술 여부\",\n",
    "    \"PGS 시술 여부\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리형 컬럼들을 문자열로 변환\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"임신 시도 또는 마지막 임신 경과 연수\",\n",
    "    \"총 생성 배아 수\",\n",
    "    \"미세주입된 난자 수\",\n",
    "    \"미세주입에서 생성된 배아 수\",\n",
    "    \"이식된 배아 수\",\n",
    "    \"미세주입 배아 이식 수\",\n",
    "    \"저장된 배아 수\",\n",
    "    \"미세주입 후 저장된 배아 수\",\n",
    "    \"해동된 배아 수\",\n",
    "    \"해동 난자 수\",\n",
    "    \"수집된 신선 난자 수\",\n",
    "    \"저장된 신선 난자 수\",\n",
    "    \"혼합된 난자 수\",\n",
    "    \"파트너 정자와 혼합된 난자 수\",\n",
    "    \"기증자 정자와 혼합된 난자 수\",\n",
    "    \"난자 채취 경과일\",\n",
    "    \"난자 해동 경과일\",\n",
    "    \"난자 혼합 경과일\",\n",
    "    \"배아 이식 경과일\",\n",
    "    \"배아 해동 경과일\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded[numeric_columns] = X_train_encoded[numeric_columns].fillna(0)\n",
    "X_test_encoded[numeric_columns] = X_test_encoded[numeric_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 훈련 데이터와 테스트 데이터를 CSV 파일로 저장\n",
    "X_train_encoded['임신 성공 여부'] = y  # 타겟 변수도 훈련 데이터에 포함시킴\n",
    "X_train_encoded.to_csv('../data/train_encoded.csv', index=False)  # 훈련 데이터 저장\n",
    "\n",
    "X_test_encoded.to_csv('../data/test_encoded.csv', index=False)  # 테스트 데이터 저장\n",
    "\n",
    "print(\"전처리된 데이터가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;ExtraTreesClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier(random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_encoded, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = model.predict_proba(X_test_encoded)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#앙상븡 모델 적용 후 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 53102, number of negative: 151978\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 716\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "Confusion Matrix\n",
      "[[35869  2276]\n",
      " [11035  2091]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84     38145\n",
      "           1       0.48      0.16      0.24     13126\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.62      0.55      0.54     51271\n",
      "weighted avg       0.69      0.74      0.69     51271\n",
      "\n",
      "예측 결과가 'baseline_submit.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 전처리된 훈련 데이터와 테스트 데이터를 파일에서 로드\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# 타겟 변수와 특성 분리\n",
    "y = X_train_encoded['임신 성공 여부']\n",
    "X = X_train_encoded.drop('임신 성공 여부', axis=1)\n",
    "\n",
    "# 훈련 데이터 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 정의 (여러 모델을 사용)\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# 앙상블 모델 정의 (Voting Classifier)\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('et', et_model),\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "], voting='soft')  # soft voting: 예측 확률의 평균으로 최종 예측\n",
    "\n",
    "# 모델 학습\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = voting_model.predict(X_val)\n",
    "y_pred_proba = voting_model.predict_proba(X_val)[:, 1]  # 예측 확률\n",
    "\n",
    "# 성능 평가\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# 예측 결과를 sample_submission 파일에 저장\n",
    "pred_proba = voting_model.predict_proba(X_test_encoded)[:, 1]\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba\n",
    "\n",
    "# 제출 파일로 저장\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "\n",
    "print(\"예측 결과가 'baseline_submit.csv'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smote 적용 코드(아주 조금 오름)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\pung\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
=======
   "outputs": [],
   "source": [
    "\n",
>>>>>>> 6bfd619 (파일 이동)
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 데이터 로드\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# 타겟 변수와 특성 분리\n",
    "y = X_train_encoded['임신 성공 여부']\n",
    "X = X_train_encoded.drop('임신 성공 여부', axis=1)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 나누기 (테스트 데이터는 원본 그대로 둠)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# SMOTE 적용 (훈련 데이터에만 오버샘플링 적용)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 데이터 크기 조정 (필요시)\n",
    "target_size = 150000\n",
    "if len(X_train_resampled) > target_size:\n",
    "    X_train_resampled = X_train_resampled[:target_size]\n",
    "    y_train_resampled = y_train_resampled[:target_size]\n",
    "\n",
    "# 개별 모델 정의\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# 개별 모델 하이퍼파라미터 튜닝\n",
    "param_grid_et = {'n_estimators': [100, 200]}\n",
    "grid_et = GridSearchCV(et_model, param_grid_et, cv=3, n_jobs=4, verbose=2)\n",
    "grid_et.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_rf = {'n_estimators': [100, 200]}\n",
    "grid_rf = GridSearchCV(rf_model, param_grid_rf, cv=3, n_jobs=4, verbose=2)\n",
    "grid_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_xgb = {'max_depth': [3, 5]}\n",
    "grid_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=3, n_jobs=4, verbose=2)\n",
    "grid_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "param_grid_lgb = {'num_leaves': [31, 50]}\n",
    "grid_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=3, n_jobs=4, verbose=2)\n",
    "grid_lgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 학습된 모델로 VotingClassifier 생성\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('et', grid_et.best_estimator_),\n",
    "    ('rf', grid_rf.best_estimator_),\n",
    "    ('xgb', grid_xgb.best_estimator_),\n",
    "    ('lgb', grid_lgb.best_estimator_)\n",
    "], voting='soft')\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "voting_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 검증 데이터 예측\n",
    "y_pred = voting_model.predict(X_val)\n",
    "y_pred_proba = voting_model.predict_proba(X_val)[:, 1]  # 예측 확률\n",
    "\n",
    "# 성능 평가\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# 테스트 데이터 예측 및 제출 파일 생성\n",
    "pred_proba = voting_model.predict_proba(X_test_encoded)[:, 1]\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba\n",
    "\n",
    "# CSV 파일로 저장\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "print(\"예측 결과가 'baseline_submit.csv'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "테스트 데이터 비율 5:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#테스트 데이터 비율 5:5"
=======
    "#머신러닝+딥러닝.7393 "
>>>>>>> 6bfd619 (파일 이동)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'임신 성공 여부'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '임신 성공 여부'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m X_train_encoded\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m임신 성공 여부\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 1. **테스트 데이터 0과 1의 개수를 1:1로 맞추기**\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m y_test \u001b[38;5;241m=\u001b[39m X_test_encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m임신 성공 여부\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 테스트 데이터의 실제 정답 (있다면 사용)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_test_encoded \u001b[38;5;241m=\u001b[39m X_test_encoded\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m임신 성공 여부\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 모델 학습을 위해 타겟 제거\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 0과 1 개수 확인\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '임신 성공 여부'"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 로드 및 전처리 완료!\n"
>>>>>>> 6bfd619 (파일 이동)
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
<<<<<<< HEAD
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
=======
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
>>>>>>> 6bfd619 (파일 이동)
    "\n",
    "# 데이터 로드\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# 타겟 변수와 특성 분리\n",
    "y = X_train_encoded['임신 성공 여부']\n",
    "X = X_train_encoded.drop('임신 성공 여부', axis=1)\n",
    "\n",
<<<<<<< HEAD
    "# 1. **테스트 데이터 0과 1의 개수를 1:1로 맞추기**\n",
    "y_test = X_test_encoded['임신 성공 여부']  # 테스트 데이터의 실제 정답 (있다면 사용)\n",
    "X_test_encoded = X_test_encoded.drop('임신 성공 여부', axis=1)  # 모델 학습을 위해 타겟 제거\n",
    "\n",
    "# 0과 1 개수 확인\n",
    "count_0 = (y_test == 0).sum()\n",
    "count_1 = (y_test == 1).sum()\n",
    "\n",
    "# 0과 1 개수를 동일하게 맞추기 위해 많은 쪽에서 일부 제거\n",
    "if count_0 > count_1:\n",
    "    idx_0 = y_test[y_test == 0].index[:count_1]  # 1과 개수 맞추기\n",
    "    idx_1 = y_test[y_test == 1].index\n",
    "elif count_1 > count_0:\n",
    "    idx_1 = y_test[y_test == 1].index[:count_0]  # 0과 개수 맞추기\n",
    "    idx_0 = y_test[y_test == 0].index\n",
    "else:\n",
    "    idx_0 = y_test[y_test == 0].index\n",
    "    idx_1 = y_test[y_test == 1].index\n",
    "\n",
    "# 균형 잡힌 테스트 데이터셋 생성\n",
    "balanced_idx = idx_0.append(idx_1)  # 0과 1이 같은 개수로 맞춰짐\n",
    "X_test_balanced = X_test_encoded.loc[balanced_idx]\n",
    "y_test_balanced = y_test.loc[balanced_idx]  # 0과 1이 1:1로 맞춰진 라벨\n",
    "\n",
    "# 2. **훈련 데이터 나누기**\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. **모델 정의**\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# 4. **앙상블 모델 정의 (Voting Classifier)**\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('et', et_model),\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "], voting='soft')\n",
    "\n",
    "# 5. **모델 학습**\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. **검증 데이터 예측**\n",
    "y_pred = voting_model.predict(X_val)\n",
    "\n",
    "# 7. **성능 평가**\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# 8. **테스트 데이터 예측**\n",
    "y_test_pred = voting_model.predict(X_test_balanced)\n",
    "\n",
    "# 9. **제출 파일 생성**\n",
    "sample_submission = pd.DataFrame({'id': balanced_idx, 'label': y_test_pred})  # 0과 1의 개수를 맞춘 테스트 데이터셋\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "\n",
    "print(\"예측 결과가 'baseline_submit.csv'에 저장되었습니다. (테스트 데이터 0과 1 비율 1:1 조정됨)\")"
=======
    "# 데이터 분할 (훈련 / 검증)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 스케일링 (딥러닝 모델을 위해 정규화)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(\"✅ 데이터 로드 및 전처리 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:42:19,047] A new study created in memory with name: no-name-405f5cf4-cac4-4c5c-800c-8640c1e021ba\n",
      "[I 2025-02-18 01:42:24,877] Trial 0 finished with value: 0.7435587369078036 and parameters: {'n_estimators': 499, 'max_depth': 4, 'learning_rate': 0.22760176235384683, 'subsample': 0.6773622284150077, 'colsample_bytree': 0.8282708277703789}. Best is trial 0 with value: 0.7435587369078036.\n",
      "[I 2025-02-18 01:42:26,501] Trial 1 finished with value: 0.7416473249985371 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.012976112657611904, 'subsample': 0.8789137814207044, 'colsample_bytree': 0.6591256477246257}. Best is trial 0 with value: 0.7435587369078036.\n",
      "[I 2025-02-18 01:42:29,839] Trial 2 finished with value: 0.7455481656296932 and parameters: {'n_estimators': 210, 'max_depth': 6, 'learning_rate': 0.03840168798357078, 'subsample': 0.7443776428294584, 'colsample_bytree': 0.6454894606692589}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:42:36,800] Trial 3 finished with value: 0.7419788964521854 and parameters: {'n_estimators': 286, 'max_depth': 10, 'learning_rate': 0.05146145991883897, 'subsample': 0.6694833751542448, 'colsample_bytree': 0.6664923336609747}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:42:41,460] Trial 4 finished with value: 0.7454506446139143 and parameters: {'n_estimators': 356, 'max_depth': 3, 'learning_rate': 0.0391008701376658, 'subsample': 0.9846831402674946, 'colsample_bytree': 0.8589328806577217}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:42:46,990] Trial 5 finished with value: 0.7447289890971505 and parameters: {'n_estimators': 329, 'max_depth': 7, 'learning_rate': 0.055051040303424406, 'subsample': 0.8602805441522539, 'colsample_bytree': 0.9360581399391654}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:42:51,520] Trial 6 finished with value: 0.7448265101129293 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.06926551039794837, 'subsample': 0.8169994125537271, 'colsample_bytree': 0.6557091459986246}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:42:57,484] Trial 7 finished with value: 0.7446509722845274 and parameters: {'n_estimators': 240, 'max_depth': 10, 'learning_rate': 0.02044392170864099, 'subsample': 0.9208445290606162, 'colsample_bytree': 0.5364713853991512}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:43:01,395] Trial 8 finished with value: 0.7394043416356225 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.17963202996174069, 'subsample': 0.8848241912516778, 'colsample_bytree': 0.7531825921142532}. Best is trial 2 with value: 0.7455481656296932.\n",
      "[I 2025-02-18 01:43:07,004] Trial 9 finished with value: 0.7458797370833414 and parameters: {'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.027462386801356306, 'subsample': 0.6783554997274879, 'colsample_bytree': 0.5150129502056162}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:12,871] Trial 10 finished with value: 0.7442998966277232 and parameters: {'n_estimators': 404, 'max_depth': 5, 'learning_rate': 0.11452457645653957, 'subsample': 0.5061384722983511, 'colsample_bytree': 0.5062176319367411}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:15,574] Trial 11 finished with value: 0.7447289890971505 and parameters: {'n_estimators': 129, 'max_depth': 6, 'learning_rate': 0.025625972803268685, 'subsample': 0.6994999179091217, 'colsample_bytree': 0.5837407563691352}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:21,377] Trial 12 finished with value: 0.743968325174075 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.010022119170151007, 'subsample': 0.5841490362331079, 'colsample_bytree': 0.5868378731452183}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:25,274] Trial 13 finished with value: 0.7458602328801857 and parameters: {'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.025719959310641013, 'subsample': 0.7746619916754525, 'colsample_bytree': 0.735615006913186}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:29,650] Trial 14 finished with value: 0.7449240311287082 and parameters: {'n_estimators': 162, 'max_depth': 8, 'learning_rate': 0.01919951587330898, 'subsample': 0.7782467076985898, 'colsample_bytree': 0.7640683130981387}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:37,268] Trial 15 finished with value: 0.7445534512687484 and parameters: {'n_estimators': 407, 'max_depth': 7, 'learning_rate': 0.02826505796677398, 'subsample': 0.631563138061815, 'colsample_bytree': 0.9766527088003636}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:40,023] Trial 16 finished with value: 0.7454116362076028 and parameters: {'n_estimators': 160, 'max_depth': 5, 'learning_rate': 0.08547361190712535, 'subsample': 0.7518282638414111, 'colsample_bytree': 0.8429226340190444}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:47,408] Trial 17 finished with value: 0.7452360983792007 and parameters: {'n_estimators': 370, 'max_depth': 8, 'learning_rate': 0.019199149891148907, 'subsample': 0.5704565260327558, 'colsample_bytree': 0.7266192143640539}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:43:52,121] Trial 18 finished with value: 0.7448265101129293 and parameters: {'n_estimators': 262, 'max_depth': 5, 'learning_rate': 0.01460251878335774, 'subsample': 0.7187902262362286, 'colsample_bytree': 0.908949679341999}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:44:01,473] Trial 19 finished with value: 0.7447289890971505 and parameters: {'n_estimators': 495, 'max_depth': 7, 'learning_rate': 0.03370040451510624, 'subsample': 0.8051376184790664, 'colsample_bytree': 0.7857751332118059}. Best is trial 9 with value: 0.7458797370833414.\n",
      "[I 2025-02-18 01:44:01,474] A new study created in memory with name: no-name-552f18d9-bdf8-4886-94df-ce3805677419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 최적 XGBoost 하이퍼파라미터: {'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.027462386801356306, 'subsample': 0.6783554997274879, 'colsample_bytree': 0.5150129502056162}\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:04,740] Trial 0 finished with value: 0.7445144428624368 and parameters: {'n_estimators': 318, 'num_leaves': 23, 'learning_rate': 0.012781345512428977, 'subsample': 0.8319339489957291, 'colsample_bytree': 0.9336002899375223}. Best is trial 0 with value: 0.7445144428624368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:09,126] Trial 1 finished with value: 0.7439488209709192 and parameters: {'n_estimators': 357, 'num_leaves': 54, 'learning_rate': 0.10616654902324366, 'subsample': 0.8143141969480132, 'colsample_bytree': 0.5663132848389543}. Best is trial 0 with value: 0.7445144428624368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:11,759] Trial 2 finished with value: 0.7431881570478438 and parameters: {'n_estimators': 116, 'num_leaves': 80, 'learning_rate': 0.012291721058868926, 'subsample': 0.7011652836313969, 'colsample_bytree': 0.9349794547880605}. Best is trial 0 with value: 0.7445144428624368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:13,791] Trial 3 finished with value: 0.7453141151918239 and parameters: {'n_estimators': 190, 'num_leaves': 31, 'learning_rate': 0.06653977401670073, 'subsample': 0.8442743828984727, 'colsample_bytree': 0.7192852825518206}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:16,526] Trial 4 finished with value: 0.7450605605507987 and parameters: {'n_estimators': 239, 'num_leaves': 48, 'learning_rate': 0.07375561376197526, 'subsample': 0.8927464844259773, 'colsample_bytree': 0.6238048012971955}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:20,164] Trial 5 finished with value: 0.7432466696573111 and parameters: {'n_estimators': 242, 'num_leaves': 99, 'learning_rate': 0.0774474459557274, 'subsample': 0.8026677353511704, 'colsample_bytree': 0.966412083524659}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:23,683] Trial 6 finished with value: 0.7418033586237834 and parameters: {'n_estimators': 364, 'num_leaves': 43, 'learning_rate': 0.13778101652677308, 'subsample': 0.6797045300183637, 'colsample_bytree': 0.7860763287022132}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:29,041] Trial 7 finished with value: 0.7451580815665776 and parameters: {'n_estimators': 329, 'num_leaves': 65, 'learning_rate': 0.0429687324546422, 'subsample': 0.6227353428083362, 'colsample_bytree': 0.6217133858904847}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:33,629] Trial 8 finished with value: 0.7448850227223967 and parameters: {'n_estimators': 319, 'num_leaves': 54, 'learning_rate': 0.015805324300529944, 'subsample': 0.7307553595777936, 'colsample_bytree': 0.785379771858352}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:38,392] Trial 9 finished with value: 0.7439878293772308 and parameters: {'n_estimators': 478, 'num_leaves': 31, 'learning_rate': 0.11570566748564609, 'subsample': 0.6798977347972878, 'colsample_bytree': 0.5944471880819544}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:39,499] Trial 10 finished with value: 0.7437732831425172 and parameters: {'n_estimators': 109, 'num_leaves': 20, 'learning_rate': 0.2762321579409702, 'subsample': 0.9858038453965211, 'colsample_bytree': 0.7151639776313418}. Best is trial 3 with value: 0.7453141151918239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:43,460] Trial 11 finished with value: 0.745567669832849 and parameters: {'n_estimators': 210, 'num_leaves': 71, 'learning_rate': 0.032664465780622425, 'subsample': 0.5340646137580991, 'colsample_bytree': 0.6879015557656992}. Best is trial 11 with value: 0.745567669832849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:47,289] Trial 12 finished with value: 0.7453336193949797 and parameters: {'n_estimators': 194, 'num_leaves': 70, 'learning_rate': 0.031875417637701485, 'subsample': 0.5005911293035638, 'colsample_bytree': 0.7088633209434267}. Best is trial 11 with value: 0.745567669832849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:51,282] Trial 13 finished with value: 0.7459577538959645 and parameters: {'n_estimators': 186, 'num_leaves': 73, 'learning_rate': 0.02908951868393986, 'subsample': 0.5119387337569103, 'colsample_bytree': 0.5013842846613132}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:55,349] Trial 14 finished with value: 0.7452751067855122 and parameters: {'n_estimators': 164, 'num_leaves': 83, 'learning_rate': 0.023670419984276443, 'subsample': 0.5076886638009425, 'colsample_bytree': 0.5025522038614554}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:44:59,682] Trial 15 finished with value: 0.745041056347643 and parameters: {'n_estimators': 255, 'num_leaves': 78, 'learning_rate': 0.025587139434280447, 'subsample': 0.5846926184894364, 'colsample_bytree': 0.8652536530304012}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:45:03,162] Trial 16 finished with value: 0.7449240311287082 and parameters: {'n_estimators': 155, 'num_leaves': 94, 'learning_rate': 0.01998471269619176, 'subsample': 0.5762922428986839, 'colsample_bytree': 0.5168480096135064}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:45:08,328] Trial 17 finished with value: 0.7444559302529695 and parameters: {'n_estimators': 437, 'num_leaves': 69, 'learning_rate': 0.04267117767817204, 'subsample': 0.5583120152122576, 'colsample_bytree': 0.6701200288437578}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:45:12,419] Trial 18 finished with value: 0.7452165941760449 and parameters: {'n_estimators': 267, 'num_leaves': 89, 'learning_rate': 0.036363909821215575, 'subsample': 0.6281555510888956, 'colsample_bytree': 0.847546135345474}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 01:45:16,868] Trial 19 finished with value: 0.743968325174075 and parameters: {'n_estimators': 208, 'num_leaves': 75, 'learning_rate': 0.010010853920743794, 'subsample': 0.5367003779902199, 'colsample_bytree': 0.5566964894800409}. Best is trial 13 with value: 0.7459577538959645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 최적 LightGBM 하이퍼파라미터: {'n_estimators': 186, 'num_leaves': 73, 'learning_rate': 0.02908951868393986, 'subsample': 0.5119387337569103, 'colsample_bytree': 0.5013842846613132}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# XGBoost 하이퍼파라미터 최적화\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # 변경됨\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # 변경됨\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)  # 변경됨\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=20)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "print(f\"🔥 최적 XGBoost 하이퍼파라미터: {best_xgb_params}\")\n",
    "\n",
    "# LightGBM 하이퍼파라미터 최적화\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # 변경됨\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # 변경됨\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)  # 변경됨\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=20)\n",
    "best_lgb_params = study_lgb.best_params\n",
    "print(f\"🔥 최적 LightGBM 하이퍼파라미터: {best_lgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35321, number of negative: 101399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 697\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258346 -> initscore=-1.054586\n",
      "[LightGBM] [Info] Start training from score -1.054586\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35321, number of negative: 101399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 696\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258346 -> initscore=-1.054586\n",
      "[LightGBM] [Info] Start training from score -1.054586\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35322, number of negative: 101398\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 136720, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258353 -> initscore=-1.054547\n",
      "[LightGBM] [Info] Start training from score -1.054547\n",
      "✅ Stacking 모델 학습 완료!\n",
      "Confusion Matrix\n",
      "[[35359  2666]\n",
      " [10460  2786]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84     38025\n",
      "           1       0.51      0.21      0.30     13246\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.64      0.57      0.57     51271\n",
      "weighted avg       0.70      0.74      0.70     51271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 개별 모델 정의\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(**best_xgb_params, random_state=42)\n",
    "lgb_model = lgb.LGBMClassifier(**best_lgb_params, random_state=42)\n",
    "\n",
    "# Stacking 앙상블 정의\n",
    "stacking_model = StackingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "], final_estimator=LogisticRegression(), cv=3)\n",
    "\n",
    "# 모델 학습\n",
    "stacking_model.fit(X_train, y_train)\n",
    "print(\"✅ Stacking 모델 학습 완료!\")\n",
    "\n",
    "# 검증 데이터 예측\n",
    "y_pred = stacking_model.predict(X_val)\n",
    "\n",
    "# 평가 지표 확인\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6863155364990234\n",
      "Epoch 10, Loss: 0.575110912322998\n",
      "Epoch 20, Loss: 0.5392276644706726\n",
      "Epoch 30, Loss: 0.5205195546150208\n",
      "Epoch 40, Loss: 0.5077341794967651\n",
      "✅ 딥러닝 모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch 모델 정의\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 데이터 변환\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"✅ 딥러닝 모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 예측 결과가 'baseline_submit.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Stacking 모델 + 딥러닝 예측 결합\n",
    "stacking_preds = stacking_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# 딥러닝 예측\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "dl_preds = model(X_test_tensor).cpu().detach().numpy().flatten()\n",
    "\n",
    "# 평균 Ensemble\n",
    "final_preds = (stacking_preds + dl_preds) / 2\n",
    "\n",
    "# 제출 파일 생성\n",
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['probability'] = final_preds\n",
    "sample_submission.to_csv('../submission/baseline_submit.csv', index=False)\n",
    "print(\"✅ 예측 결과가 'baseline_submit.csv'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 최종 모델의 Confusion Matrix 🔥\n",
      "[[36884  1141]\n",
      " [11878  1368]]\n",
      "\n",
      "🔥 Classification Report 🔥\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85     38025\n",
      "           1       0.55      0.10      0.17     13246\n",
      "\n",
      "    accuracy                           0.75     51271\n",
      "   macro avg       0.65      0.54      0.51     51271\n",
      "weighted avg       0.70      0.75      0.68     51271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 검증 데이터에서 Stacking 모델 예측\n",
    "stacking_preds_val = stacking_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 검증 데이터에서 딥러닝 모델 예측\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "dl_preds_val = model(X_val_tensor).cpu().detach().numpy().flatten()\n",
    "\n",
    "# 최종 예측값 (Stacking + 딥러닝 평균)\n",
    "final_preds_val = (stacking_preds_val + dl_preds_val) / 2\n",
    "final_preds_binary = (final_preds_val >= 0.5).astype(int)  # 0.5 이상이면 1, 아니면 0\n",
    "\n",
    "# Confusion Matrix 계산\n",
    "conf_matrix = confusion_matrix(y_val, final_preds_binary)\n",
    "print(\"🔥 최종 모델의 Confusion Matrix 🔥\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report 출력\n",
    "print(\"\\n🔥 Classification Report 🔥\")\n",
    "print(classification_report(y_val, final_preds_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2월 18일 정확도를 높이기 위한 새로운 모델 시도 가자링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "\n",
    "# 데이터 로드\n",
    "X_train_encoded = pd.read_csv('../data/train_encoded.csv')\n",
    "X_test_encoded = pd.read_csv('../data/test_encoded.csv')\n",
    "\n",
    "# 타겟 변수와 특성 분리\n",
    "y = X_train_encoded['임신 성공 여부']\n",
    "X = X_train_encoded.drop('임신 성공 여부', axis=1)\n",
    "\n",
    "# 데이터 분할 (훈련 / 검증)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n",
      "[LightGBM] [Info] Start training from score -1.054573\n",
      "Confusion Matrix\n",
      "[[36974  1051]\n",
      " [12000  1246]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.97      0.85     38025\n",
      "           1       0.54      0.09      0.16     13246\n",
      "\n",
      "    accuracy                           0.75     51271\n",
      "   macro avg       0.65      0.53      0.51     51271\n",
      "weighted avg       0.70      0.75      0.67     51271\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# X_test에 대한 예측 수행\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m rf_preds_test \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     60\u001b[0m xgb_preds_test \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     61\u001b[0m lgb_preds_test \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=215,\n",
    "    max_depth=7,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=8,\n",
    "    max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=359,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.045093620098282834,\n",
    "    subsample=0.6669970717173888,\n",
    "    colsample_bytree=0.934119080218835,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=492,\n",
    "    num_leaves=88,\n",
    "    learning_rate=0.014666949438705097,\n",
    "    subsample=0.8766412273868395,\n",
    "    colsample_bytree=0.5848756135604947,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 각 모델 학습\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 각 모델의 예측을 얻기\n",
    "rf_preds = rf_model.predict_proba(X_val)[:, 1]\n",
    "xgb_preds = xgb_model.predict_proba(X_val)[:, 1]\n",
    "lgb_preds = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 각 모델에 가중치 부여 (예: rf = 0.4, xgb = 0.3, lgb = 0.3)\n",
    "rf_weight = 0.4\n",
    "xgb_weight = 0.3\n",
    "lgb_weight = 0.3\n",
    "\n",
    "# 가중치 합산\n",
    "final_preds = rf_weight * rf_preds + xgb_weight * xgb_preds + lgb_weight * lgb_preds\n",
    "\n",
    "# 예측을 확률로 처리하여 이진 예측을 도출\n",
    "final_preds_binary = (final_preds >= 0.5).astype(int)\n",
    "\n",
    "# 평가 지표 확인\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_val, final_preds_binary))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_val, final_preds_binary))\n",
    "\n",
    "import os\n",
    "\n",
    "# X_test에 대한 예측 수행\n",
    "rf_preds_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "lgb_preds_test = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 가중 평균 계산\n",
    "final_preds_test = rf_weight * rf_preds_test + xgb_weight * xgb_preds_test + lgb_weight * lgb_preds_test\n",
    "\n",
    "# 예측을 확률로 처리하여 이진 예측 도출\n",
    "final_preds_binary_test = (final_preds_test >= 0.5).astype(int)\n",
    "\n",
    "# 제출 파일 로드\n",
    "submission_path = '../submission/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(submission_path)\n",
    "sample_submission['probability'] = final_preds_test\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_path = '../submission/baseline_submit3.csv'\n",
    "sample_submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"예측 결과가 '{output_path}'에 저장되었습니다.\")\n"
>>>>>>> 6bfd619 (파일 이동)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.3"
=======
   "version": "3.12.2"
>>>>>>> 6bfd619 (파일 이동)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
